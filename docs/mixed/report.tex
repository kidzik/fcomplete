\documentclass[preprint]{imsart}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb} 
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{mathtools}  
%\usepackage[OT1]{fontenc} 
\usepackage{amsmath}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{natbib}
\bibliographystyle{imsart-nameyear}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cMN}{\mathcal{MN}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\tr}[1]{{\textcolor{red}{#1}}}
\newcommand{\tb}[1]{{\textcolor{blue}{#1}}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\ba}{\mathbf{a}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\rank}{rank}
\DeclareMathOperator*{\Cov}{Cov}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\minimize}{minimize\ \ }
\DeclareMathOperator*{\subjectto}{subject\ to\ \ }
\DeclareMathOperator*{\E}{\mathbb{E}}

\newtheorem{observation}{Observation}
\newtheorem{conjecture}{Conjecture}

\endlocaldefs

%\usepackage[backend=biber]{biblatex}
%\addbibresource{report.bib}
%\DeclareNameAlias{default}{last-first}

\begin{document}

\begin{frontmatter}
\title{Mixed-effect models\\using matrix factorization}
%% \runtitle{Longitudinal data analysis}


%%\begin{aug}
%% \author{\fnms{Trevor} \snm{Hastie}\ead[label=e1]{hastie@stanford.edu}}%\thanksref{t1}
%% \and
%% \author{\fnms{\L ukasz} \snm{Kidzi\'nski}\ead[label=e2]{lukasz.kidzinski@stanford.edu}}%\thanksref{t1}

%% \thankstext{t1}{Research supported by the Mobilize Center, a National Institutes of Health Big Data to Knowledge (NIH BD2K) Center of Excellence supported through Grant U54EB020405.}
%% \runauthor{Trevor Hastie and {\L}ukasz Kidzi\'nski}

%% \affiliation{Stanford University}

%% \address{Stanford University\\
%% \printead{e1,e2}}
%\address{\thanksmark{m1}Department of Statistics, Stanford University}%\\\printead{e1}}
%\address{\thanksmark{m2}Department of Bioengineering, Stanford University}%\\\printead{e2}}

%% \end{aug}
%% \begin{keyword}[class=MSC]
%% \kwd[Primary ]{62H12}
%% %\kwd{60K35}
%% \kwd[; secondary ]{62P10}
%% \end{keyword}

%% \begin{keyword}
%% \kwd{multivariate longitudinal data}
%% \kwd{functional data analysis}
%% \kwd{matrix factorization}
%% \kwd{regression}
%% \kwd{interpolation}
%% \end{keyword}

\end{frontmatter}

\maketitle
%% \section{Introduction}

%% Let $Y$ be an $n \times k$ matrix of observations and $X$ be an $n \times q$ matrix of covariates. We assume that $Y$ is correlated with $X$ and that it has a low-rank structure, i.e. we assume that
%% \[
%% Y \sim WA + XB
%% \]

\section{Mixed effect model}
Suppose we have $n$ individuals and we measure some $k$ parameters. Let $\by_{i,j}$ be a $j$-th parameter measured for $i$-th individual. We assume a mixed-effect model
\begin{align}\label{eq:mm}
\by_{i,j} = \ba_j\bx_{i} + \bb_j\bw_i + \varepsilon_{i,j},
\end{align}
where $\bx_{i} \in \R^p$ are known covariates associated with the individual $i$, $\bw_{i} \in \R^q$ is an unknown random-effect and $\ba_j \in \R^p$ and $\bb_j \in \R^q$ are unknown coefficients. We assume that $\varepsilon_{i,j}$ are independent $\cN(0,\sigma^2)$ variables, $\E \bw_i = 0$, and $\Var(\bw_i) = \Sigma$ with unknown $\Sigma$. In particular, we have $\E y_i = A \bx_i $.

For each individual $i$, the observation vector $\by_{i} \in \R^K$ of $k$ measured parameters is given by
\[
\by_{i} = A\bx_{i} + B\bw_i + \varepsilon_{i,\cdot},
\]
where $A$ is a $k \times p$ matrix and $B$ is a $k \times q$ matrix. Finally, in the matrix notation we have
\begin{align}\label{eq:matrix}
Y = XA' + WB' + E,
\end{align}
where $A,B,E,W$ and $\Sigma$ are unknown. Since $E$ is zero-mean we attempt to find model parameters by optimizing
\begin{align}\label{eq:mm-optimization}
\argmin_{A,B,W} \|Y - XA' - WB'\|^2_F + \lambda\|W\|_*.
\end{align}

\begin{conjecture}
The penalty term makes \eqref{eq:mm-optimization} identifieble. For $\lambda = 0$, we have multiple solutions. Assume that $q=p$ and $A = \tilde{A}, B = 0$ minimizes \eqref{eq:mm-optimization}. Then $A = 0, W = X, B = \tilde{A}$ also minizes \eqref{eq:mm-optimization}.
\end{conjecture}

%% \begin{observation}
%% Note that this formulation allows for multiple number observations of the same parameter per subject. We just stack them in a vector $\by_i$, construct the matrix $Y$ as in \eqref{eq:matrix} and we assume we only observe indices $\Omega \subset N \times k$. Then, we optimize the sparse version of \eqref{eq:mm-optimization}, i.e.
%% \begin{align}\label{eq:mm-optimization-sparse}
%% \argmin_{A,B,W} \| P_\Omega(Y - XA' - WB') \|^2_F + \lambda\|W\|_*,
%% \end{align}
%% where $P_\Omega$ is a projection on $\Omega$ as in the previous paper. In this case, the columns in $A$ and $B$ have all coefficients equal.
%% \end{observation}

\subsection{Solution}

\begin{conjecture}\label{cj:solution}
If $A$ was known we can find $WB'$ by taking SVD of $Y - XA'$ and truncating it to the desired rank. If $WB'$ was known we can find $A$ by linear regression of $Y - WB'$ on $X$. By starting from a randomly assinged values and  iteating we converge to a solution.
\end{conjecture}

\begin{conjecture}
Let $A,B,W,\Sigma$ be a solution of the mixed effect model \eqref{eq:mm}. Then $A,B,W$ minimizes \eqref{eq:mm-optimization}? %[Figure out the link with the Gauss-Markov theorem]
\end{conjecture}
%\tableofcontents

\end{document}
