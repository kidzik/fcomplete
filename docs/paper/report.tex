\documentclass[preprint]{imsart}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb} 
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{mathtools}  
%\usepackage[OT1]{fontenc} 
\usepackage{amsmath}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{natbib}
\bibliographystyle{imsart-nameyear}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cMN}{\mathcal{MN}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\tr}[1]{{\textcolor{red}{#1}}}
\newcommand{\tb}[1]{{\textcolor{blue}{#1}}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bB}{\mathbf{B}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\rank}{rank}
\DeclareMathOperator*{\Cov}{Cov}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\minimize}{minimize\ \ }
\DeclareMathOperator*{\subjectto}{subject\ to\ \ }
\DeclareMathOperator*{\E}{\mathbb{E}}

\endlocaldefs

%\usepackage[backend=biber]{biblatex}
%\addbibresource{report.bib}
%\DeclareNameAlias{default}{last-first}

\begin{document}

\begin{frontmatter}
\title{Longitudinal data analysis\\using matrix factorization\thanksref{t1}}
\runtitle{Longitudinal data analysis}


\begin{aug}
\author{\fnms{Trevor} \snm{Hastie}\ead[label=e1]{hastie@stanford.edu}}%\thanksref{t1}
\and
\author{\fnms{\L ukasz} \snm{Kidzi\'nski}\ead[label=e2]{lukasz.kidzinski@stanford.edu}}%\thanksref{t1}

\thankstext{t1}{Research supported by the Mobilize Center, a National Institutes of Health Big Data to Knowledge (NIH BD2K) Center of Excellence supported through Grant U54EB020405.}
\runauthor{Trevor Hastie and {\L}ukasz Kidzi\'nski}

\affiliation{Stanford University}

\address{Stanford University\\
\printead{e1,e2}}
%\address{\thanksmark{m1}Department of Statistics, Stanford University}%\\\printead{e1}}
%\address{\thanksmark{m2}Department of Bioengineering, Stanford University}%\\\printead{e2}}

\end{aug}

\begin{abstract}
  In clinical practice and biomedical research measurements are often collected sparsely and irregularly in time while the data acquisition is expensive and inconvenient. Examples include measurements of spine bone mineral density, cancer growth through mammography or biopsy, progression of defect of vision, or assessment of gait in patients with neurological disorders. Since the data collection is often costly and inconvenient estimation of progression from sparse observations is of great interest for practitioners.

  From the statistical standpoint, such data is often analyzed in the context of a mixed-effect model where time is treated as both random and fixed effect. Alternatively, researchers analyze gaussian processes or functional data where observations are assumed to be drawn from a certain distribution of processes. These models are flexible but rely on a large set of probabilistic assumptions and require careful deployment.
  
  In this study, we propose an alternative elementary framework for analyzing longitudinal data, relying on matrix factorization. Our method yields point estimates of progression curves by iterative application of the SVD. Our framework covers multivariate longitudinal data, regression and can be easily extended to other settings.

  We apply our methods to understand trends of progression of motor impairment in children with Cerebral Palsy. Our model approximates individual progression curves and explains 30\% of variability. Low rank representation of progression trends enable discovering that subtypes of impairment exhibit different progression trends. 
  
\end{abstract}

\begin{keyword}[class=MSC]
\kwd[Primary ]{62H12}
%\kwd{60K35}
\kwd[; secondary ]{62P10}
\end{keyword}

\begin{keyword}
\kwd{multivariate longitudinal data}
\kwd{functional data analysis}
\kwd{matrix factorization}
\kwd{regression}
\kwd{interpolation}
\end{keyword}

\end{frontmatter}

\maketitle

%\tableofcontents

\section{Motivation}

One of the fundamental questions in medical practice is how diseases progress in individual patients. Accurate continuous monitoring of patient's condition could considerably improve prevention and treatment. Many medical tests, such as x-ray, MRI, motion capture gait analysis, biopsy, or blood tests are costly, harmful or inconvenient to perform frequently. Since in most situations increase in sampling is not feasible due to inconvenience and costs, practitioners need to reach out for statistical tools to analyze dynamics of these measurements.
% In practice, due to inherent inconvenience and costs of these exams, measurements are taken sparsely in time, tolerating potential complications between the visits.

While in many situations multiple data points from patients' histories are available, these data are often underutilized. For the sake of simplicity and convenience, many prognostic models applied in practice only use the last few observations from a series or summary statistics such as the mean over time. However, this simplification ignores important information about the progression, including its dynamics or individual patient's variability. Moreover, the noise inherent to measurements further hinders the inference of changes in patient's health. For making use of these data, practitioners need statistical models and software. To enable appropriate usage and broad adoption, ideally, these tools should be simple to use and understand.

%This motivates research on the longitudinal statistical analysis of progression, recovering trajectories from sparse observations. 

To illustrate potential benefits of temporal models in these scenarios, in Figure \ref{fig:motivation}, we present measurements of BMI of patients with gait pathologies. The thick solid line represents the estimated mean and indicates a clear trend of growth during puberty. However, by looking at individual processes and by modeling between-subject similarities, we may model the progression more accurately. This could result in personalized predictions, new clustering techniques patients based on progression patterns, or extraction of latent representation of progression patterns, which then could be used as predictors in other models. 

\begin{figure}[h]
  \includegraphics[width=0.49\linewidth]{../plots/points}
  \includegraphics[width=0.49\linewidth]{../plots/grouped}
  \caption{We observe BMI of patients at every visit (left plot), and we can easily derive the population progression (the thick solid curve). The analysis of individual patients (connected dots in the right plot, also differing by color) can reveal patterns in individual trends.}
  \label{fig:motivation}
\end{figure}

This kind of data has been commonly analyzed using linear mixed models, where we treat time as a random effect and nonlinearity of this effect is imposed by the choice of the functional basis \citep{zeger1988models, verbeke1997linear, mcculloch2001generalized}. When data is very sparse, additional constraints on covariance structure of trajectories are derived using cross-validation or information criteria \citep{rice2001nonparametric,bigelow2009bayesian}. To further reduce the dimension, practitioners model the covariance as a low-rank matrix \citep{james2000principal,berkey1983longitudinal, yan2017dynamic, hall2006properties, besse1986principal, yao2006penalized, greven2011longitudinal}. Multiple models were developed for incorporating additional covariates \citep{song2002semiparametric, liu2009joint, rizopoulos2014combining}. While these methods are implemented in practice, the nature of processes differs in each clinical setting. Moreover, the probabilistic formulation of the model and dependence on underlying distributions might hinder applicability or adoption to other practical problems.

In this work, we propose an alternative elementary and flexible statistical framework, exploiting matrix factorization techniques \citep{mazumder2010spectral, hastie2015matrix, fazel2002matrix, cai2010singular}. We focus on simplicity of the formulation, and we implement software easy to use and extend. 

\section{Background and related work}\label{s:background}

In many clinical settings, researchers and practitioners model patient's history as a multivariate process of clinical measurements (such as the size of a tumor, blood markers, height and weight of a patient, etc.). The multivariate measurements are noisy, and they are observed on different time-points. This framework includes cases when, for example, clinicians perform a blood test at every visit but a biopsy sporadically. 

Even though multiple variables are measured, the ones directly related to the disease, such as the size of a tumor, are usually of the special interest. Therefore, to focus our attention and aid further discussion, we start by introducing notation and methodology for the univariate case. Let $N$ denote the number of subjects. For each individual $i \in \{ 1,2,...,N \}$, we measure the process at $n_i$ irregularly sampled time-points $\mathbf{t}_i = [t_{i,1},t_{i,2},...,t_{i,n_i}]'$. We assume that there are some $t_{min},t_{max}$ and that for each $i$ we have $t_{min} < t_{i,1} < t_{i,2} < ... < t_{i,n_i} < t_{max}$.
%Since the time interval can be easily transformed linearly, without loss of generality we assume that $t_{i,j} \in (t_{\min},t_{\max})$ for $ j \in \{1,...,n_i\}$ and some $t_{\min} < t_{\max}$.

Let $\by_i = [y_{i,1,},...,y_{i,n_i}]'$ denote observations corresponding to $\bt_i$ for each individual $i$. To model individual trajectories given pairs $(\mathbf{t}_i,\mathbf{y}_i)$ practitioners map observations into a low-dimensional space which represents progression patterns. Ideally, a small distance between individuals in the latent space reflects similar progression patterns.

In this section, we discuss state-of-the-art approaches to estimating this low-dimensional latent embedding. We classify them into three categories: the direct approach, mixed-effect models, and low-rank approximations. 

\subsection{Direct approach}\label{ss:direct}

If the set of observed values for each individual is dense, elementary interpolation using a continuous basis can be sufficient for approximating the entire trajectory. Let $\{b_i: i \in \N \}$ be a basis of $L_2([t_{\min},t_{\max}])$. In practice, we truncate the basis ta a set of the first $K \in \N$ basis elements. Let $\bb(t) = [b_1(t),b_2(t),...,b_K(t)]'$ be a vector of $K$ basis elements evaluated at a timepoint $t \in (t_{\min},t_{\max})$. Throught this article we use the word \emph{basis} to refer to some truncated basis of $K$ elements.

To find an individual trajectory, for each subject $i \in \{ 1,...,N \}$, we might use least squares method to find a set of coefficients $\bw_i \in \R^K$, minimizing squared euclidean distance to the observed point
\begin{align}\label{eq:direct-individual}
 \argmin_{\bw_i}\sum_{j=1}^{n_i}\left|y_{i,j} - \bw_i'\bb(t_{i,j})\right|^2.
\end{align}

Finally, to reduce the potential overfitting, it is often convenient to compute principal functional components across all individuals and represent curves in the space spanned by the first few of them. Such representation, referred to as a {\it Karhunen-Lo\`eve} expansion \citep{watanabe1965karhunen,kosambi2016statistics}, has became a foundation of many functional data analysis workflows \citep{ramsay1991some,yao2005linear,cnaan1997tutorial,laird1988missing,horvath2012inference,besse1997simultaneous}. %If measurements are dense, confidence in estimates of $\bw_i$ is high and therefore the sample covariance of $\bw_i$ might also be an accurate estimator. hormann2015dynamic,

This approach to modeling covariance has two main drawbacks. First, if the number of observations $n_i$ for an individual $i$ is smaller or equal to the size of the basis $K$, we can fit a curve with no error leading to overfitting and unreliable estimator of the variance. Second, this approach ignores similarities between the curves, which could potentially improve the fit.

A basic idea to remedy these issues is to estimate both the basis coefficients and the variance structure simultaneously within the framework of linear mixed-effect models.

\subsection{Linear mixed-effect models}\label{ss:lmm}

A common approach to modeling longitudinal observation $(\mathbf{t}_i, \mathbf{y}_i)$ is to assume that data come from a linear mixed-effect model (LMM) \citep{verbeke1997linear, zeger1988models}. We operate in a functional basis $\bb(t)$ of $K$ elements and we assume there exists a fixed effect $\mu(t) = m' \bb(t)$, where $m = [m_1,...,m_K]'$ for $m_i \in \R$. We model the individual random effect as a vector of basis coefficients. In the simplest form, we assume
\begin{align}\label{eq:latent-probabilistic}
 \mathbf{w}_i \sim \cN(0, \Sigma),
\end{align}
where $\Sigma$ is a $K \times K$ covariance matrix. We model individual observations as
\begin{align}\label{eq:probabilistic}
 \mathbf{y}_i|\mathbf{w}_i \sim \cN(\mu_i + B_i\mathbf{w}_i, \sigma^2I_{n_i}),
\end{align}
where $\mu_i = [\mu(t_{i,1}),\mu(t_{i,2}),...,\mu(t_{i,n_i})]'$, $\sigma$ is the standard deviation of observation error and $B_i = [\bb(t_{i,1}),...,\bb(t_{i,n_i})]'$ is the basis evaluated at timepoints defined in $\bt_i$. Estimation of the model parameters is typically accomplished by the expectation-maximization (EM) algorithm \citep{laird1982random}. For estimating coefficients $\bw_i$ one can use the best unbiased linear predictor (BLUP) \citep{henderson1950estimation,robinson1991blup}. % or other techniques typical for mixed-effect models \citep{mardia1980multivariate}.

Since the LMM estimates the covariance structure and individual fit simultaneously, it reduces the problem of uncertain estimates of $\bw_i$, present in the direct approach. However, this model is only applicable if a relatively large number of observations per subject is observed since we attempt to estimate $K$ basis coefficients for every subject.

To model trajectories from a small number of observations, practitioners further constrain the covariance structure. If we knew the functions which contribute the most to the random effect, we could fit a LMM in a smaller space spanned by these functions. In the next section we explore possibilities to learn the basis from the data. %A solution is to learn the basis from the data, by constraining the rank of the covariance matrix or by imposing a prior on the basis parameters.

\subsection{Low-rank approximations}\label{ss:reduced-rank}

There are multiple ways to constrain the covariance structure. We can use cross-validation or information criteria to choose the best basis, the number of elements or positions of spline knots \citep{rice2001nonparametric,bigelow2009bayesian}. Alternatively we can place a prior on the covariance matrix \citep{maclehose2009nonparametric}.

Another solution is to restrict the latent space to $q < K$ dimensions and learn from the data the mapping $A \in \R^{K \times q}$ between the latent space and the basis. In the simplest scenario with Gaussian observation noise, observations are modeled as
\begin{align}\label{eq:james-model}
 \mathbf{y}_i \sim \cN(\mu_i + B_i A \mathbf{w}_i, \sigma^2I_{n_i}).
\end{align}
%In this setting $\bb(t)A$
%Note that with the freedom in the choice of $A$, one can assume that $\mathbf{w}_i\ \sim\ \cN(0,I_q)$.

\citet{james2000principal} propose an EM algorithm for finding model parameters and latent variables $\bw_i \in \R^q$. In the expectation stage, they marginalize $\bw_i$, while % given the model, i.e. $\mathbf{w}_i~=~A' B_i' (\mathbf{y}_i - \mu_i)$, while
In the maximization stage, with $\bw_i$ assumed observed, they maximize the likelihood with respect to $\{\mu,A,\sigma\}$. The maximum likelihood, given $\bw_i$, takes form
\begin{align*}
\prod_{i=1}^N \frac{1}{(2\pi)^{n_i/2} \sigma^{n_i} |\Sigma|^{1/2}} \exp\{ &-(\by_i - \mu_i - B_i A \bw_i)'(\by_i - \mu_i - B_i A \bw_i) / 2\sigma^2 \nonumber\\
&- \frac{1}{2}\bw_i' \Sigma^{-1} \bw \}.\label{eq:likelihood}
\end{align*}

Another approach to estimating parameters of \eqref{eq:james-model} is to optimize over $\bw_i$ and marginalize $A$ \citep{lawrence2004gaussian}. This approach allows modifying the distance measure in the latent space, using the ``kernel trick'' \citep{schulam2016disease}.

Estimation of the covariance structure of processes is central to estimation of individual trajectories. \citet{descary2016functional} propose a method where the estimate of covariance matrix is obtained through matrix completion.

Methods based on low-rank approximations are widely adopted and applied in practice \citep{berkey1983longitudinal, yan2017dynamic, hall2006properties, besse1986principal, yao2006penalized, greven2011longitudinal}. However, due to their probabilistic formulation and reliance on the distribution assumptions, these models usually need to be carefully fine-tuned for specific situations.

%To illustrate this problem in an elementary setting, let us assume that the data is drawn from two distributions with equal probability taking form \eqref{eq:james-model}, but with two different means: $\mu$ and $-\mu$. Then, the estimated fixed effect will be close to $0$ and the zero-mean prior distribution on the latent variables will draw these variables towards $0$.

%Linear Mixed Models \citep{zeger1988models, verbeke1997linear}
%If we assume that the processes come from the same distribution, we can use mixed effect models, treating time as a non-linear random effect \citep{}. Both methods require fairly large number of observation per individual. At the very least, we need to observe more points per subject than the size of the basis.
%Estimation of the {\it Karhunen-Lo\`eve} expansion becomes difficult when the data is sparsely observed.
%The functional principal components for sparse data, introduced by \citep{james2000principal}, is a parametric method that remedies problems of the direct approach. Similarly as in the direct approach, it imposes continuity by assuming that observations come from underlying continuous processes, however, since the dimensionality of the basis might still be too large for estimation, the model further constrains the space to just a few principal components driving the variability of observed processes.
%This leads to %Assuming a normal prior $\cN(0, \alpha I_q$ over rows of $A$, yields 
%\[ \mathbf{y}_i \sim \cN(\mu_i, \alpha \langle \mathbf{w}_i, \mathbf{w}_i \rangle B_i B_i' + \sigma^2I_{n_i}). \]
% Maximizing this expression is equivalent \citep{james2000principal} to minimizing
% \begin{align*}
% \sum_{i=1}^N \left\{ (Y_i - \alpha_iV M)'(Y_i - \alpha_iV M) + \sigma^2 \sum_{j=1}^k \frac{\alpha_{i,j}^2}{D_{jj}}\right\} =&\\
% \sum_{i=1}^N \| Y_i - \alpha_iV M\|^2 + \sigma^2 \sum_{j=1}^k \frac{1}{D_{jj}}\sum_{i=1}^N\alpha_{i,j}^2 =&\\
% \| Y - AV M\|_F^2 + \sigma^2 \| A D^{-1/2} \|_F^2,
% \end{align*}
% where $A = [\alpha_1,\alpha_2,...,\alpha_N]'$.
% Thus, the algorithm solves
% \begin{align}\label{eq:optpca}
% \argmin_{\sigma,A,V}\| Y M' - AV\|_F^2 + \sigma^2 \| A D^{-1/2} \|_F^2.
% \end{align}
% This method essentially projects $Y$ to a low-dimensional latent space. It motivates another approach, based on matrix factorization. We may consider \eqref{eq:direct} as a low-rank matrix approximation, with a constrain on $\rank{(A)}$, i.e.
% \begin{gather*}
% \text{minimize } \| P_\Omega(X - AM') \|^2,\\
% \text{subject to } \rank{(A)} \leq k,
% \end{gather*} 
% for some positive integer $k$. In Section \ref{ss:matrix-factorization} we introduce methods for solving this problem from matrix-factorization perspective. We further explore similarities between matrix factorization and sparse functional principal components in Section~\ref{s:the-link}. For details on sparse principal component analysis approach we refer the reader to \citep{james2000principal,yao2005linear, yao2005sparse}.


%\subsection{Cluster-based representation}
%\citep{marshall2000linear}.



%%%%%%%%%%%%%%%%%%%%%%%%%%

% Multiple methods were introduced for modeling trajectories underlying sparsely measured observations. If sufficient number of measurements per individual is observed, we can represent each trajectory in a smooth basis, such as splines. \citep{nickerson2013quantifying}



%Whenever multiple processes are measured, we can expect that they correlate and additional information can improve prediction of the main process of interest. We can consider this scenario as a functional regression setting with sparsely observed data \citep{hall2008modelling}. In such case, the mixed model is often chosen in practice \citep{cnaan1997tutorial,laird1988missing}. However, as in the univariate process case, it may require many observations per patient.

%%%%The rest of the article is organized as follows. In Section \ref{s:context}, our method is juxtaposed with a probabilistic approach introduced in \citep{james2000principal}.

%%%%For simplicity we assume that the population mean is known and we focus on second order properties. For methods o the mean see \citet{rice1991estimating}.

\section{Modeling sparse longitudinal processes using matrix factorization}\label{s:context}

%While low-rank mixed models, as presented in \citep{james2000principal,tipping1999probabilistic}, can be potentially extended to the regression setting, in this paper 
%[TODO: Describe the problem with existing approaches]

The mixed-effect model, as any other probabilistic model, can be heavily biased when the data comes from a distribution considerably different than assumed. In the medical context, since biomarkers can differ in every clinical setting, fine-tuning the models may require extensive amount of time and expertise. In this work, we develop a more flexible approach based solely on the $\ell_2$ approximation rather than the underlying probabilistic distributions.

We pose the problem of trajectory prediction as a matrix completion problem and we solve it using sparse matrix factorization techniques \citep{rennie2005fast, candes2009exact}. In the classical matrix completion problem the objective is to predict elements of a sparsely observed matrix using its known elements, while minimizing a certain criterion, often chosen to be the Mean Squared Error (MSE). The motivating example is the ``Netflix Prize'' competition \citep{bennett2007netflix}, where participants were tasked to predict unknown movie ratings using other observed ratings. We can represent these data as a matrix of $N$ users and $M$ movies, with a subset of known elements, measured on a fixed scale, e.g. $1-5$.

To solve the matrix completion problem, we normally assume that the true matrix can be approximated by a low-rank matrix \citep{srebro2005generalization}. Given the low-rank representation $WA'$, columns $A$ spanning the space of movies can be interpreted as ``genre'' components and each user is represented as a weighted sum of their preferred generes, i.e. a row in the matrix of latent variables $W$ (see Figure \ref{fig:idea}).

We can use the same idea to predict sparsely sampled curves, as long as we introduce an additional smoothing step. The low-dimensional latent structure now corresponds to progression patterns and a trajectory of each individual can be represented as a weighted sum of these ``principal'' patterns (see the matrix $B$ in Figure \ref{fig:idea}). 

%%%% The smoothness comes from the fact that in the longitudinal analysis columns of the matrix are very strongly correlated, due to continuity of the sampled processes. Conversely, as the density of the evaluation grid increases, classical matrix completion problems become more difficult, as it does not use information embedded in neighboring columns. The methodology introduced in this paper deals with this problem by projecting the observed data to a low-dimensional space of coefficients in a basis of continuous functions. In this smaller space, one can directly apply matrix completion methodology. In this scenario, dependence of the size of the grid becomes small.

\begin{figure}[h]
  \includegraphics[width=1\linewidth]{images/intro}
  \caption{The key observation motivating this paper is the fact that sparse longitudinal trajectory prediction problem can be mapped to the matrix completion problem. Matrix completion can be approached with matrix factorization where we look for $W$ and $A$ of low rank, approximating observed values in $Y$ (circled green rectangles in the matrix $Y$). In the sparse longitudinal setting, we impose continuity by fixing the basis $B$ (e.g. splines) and again we find low-rank $W$ and $A$ approximating observed values in $Y$. $B$ corresponds to some discretized smooth basis, here with $3$ basis elements.}
  \label{fig:idea}
\end{figure}

We first introduce methodology for univariate sparsely-sampled processes. The direct method, mixed-effect models and low-rank approximations described in Section \ref{s:background} have their analogy in the matrix completion setting. We discuss these analogies in sections \ref{ss:direct-matrix} and \ref{ss:low-rank-matrix}. Next, we show that the elementary representation of the problem allows for simple extension to multivariate sparsely-sampled processes and to a regression setting.

\subsection{Notation}

%To use matrix factorization techniques, in this section we represent the data as a matrix of $N$ individuals and $T$ time-points, unlike the previous work on longitudinal modeling, presented in Section \ref{s:background}. 
%As presented in Section \ref{s:background},
%The theory of functional data analysis can be developed directly on $L_2([t_{\min},t_{\max}])$ curves or, more broadly, any separable Hilbert space \citep{ferraty2006nonparametric}. In this article

%As we will move from true observations to their closes neighbours on the grid, in this chapter we denote the true ones with the tilda.
For each individual $i \in \{1,2,...,N\}$ we observe $n_i$ measurements $\{\tilde y_{i,1},...,\tilde y_{i,n_i}\}$ at time-points $\{t_{i,1},t_{i,2},...,t_{i,n_i}\}$. Unlike the prior work introduced in Section \ref{s:background}, we discretize the time grid to $T$ equidistributed time-points $G = \left[\tau_1, \tau_2, ..., \tau_T\right],$ where $t_{\min} = \tau_1$, $t_{\max} = \tau_T$ and $T$ is arbitrarly large. Each individual $i$ is expressed as a partially observed vector $r_i \in \R^T$. For each time-point $t_{i,j}$ for $1 \leq j \leq n_i$ we find a corresponding grid-point $g_i(j) = \argmin_{1 \leq k \leq T}  |\tau_k - t_{i,j}|$. We assign $y_{i,g_i(j)} = \tilde y_{i,j}$. Let $O_i = \{g_i(j): 1 \leq j \leq n_i \}$ be a set of grid indices corresponding to observed grid-points for an individual $i$. All elements outside $O_i$, i.e. $\{y_{i,j} : j \notin O_i\}$ are considered missing.

%In Section \ref{ss:direct-matrix} we show that the grid can be arbitrarily dense with a marginal compromise on performance, and therefore, without loss of generality, we assume that at most one measurement per subject is mapped to every grid-point in $G$.

For $T$ sufficiently large, our observations can be uniquely represented as a $N \times T$ matrix $Y$ with missing values. We denote the set of all observed elements by pairs of indices as $\Omega = \{ (i,j) : i\in \{1,2,...,N\}, j \in O_i \}$. Let $P_\Omega(Y)$ be the projection onto observed indices, i.e. $P_\Omega(Y) = W$, such that $W_{i,j} = Y_{i,j}$ for $(i,j) \in \Omega$ and $W_{i,j} = 0$ otherwise. We define $P^\perp_\Omega(Y)$ as the projection on the complement of $\Omega$, i.e. $P^\perp_\Omega(Y) = Y - P_\Omega(Y)$. We use $\|\cdot\|_F$ to denote the Frobenius norm, i.e. the square root of the sum of matrix elements, and $\|\cdot\|_*$ to denote the nuclear norm, i.e. the sum of singular values.

As in Section \ref{s:background} we impose smoothness by using a continuous basis $\bb(t) = [b_1(t),b_2(t),...,b_K(t)]'$. However, now, the basis is evaluated on the grid $G$ and we define $B = [\bb(\tau_1),\bb(\tau_2),...,\bb(\tau_T)]'$. %Throughout the paper we will use 

In our algorithms we use a diagonal-thresholding operators defined as follows. Let $D = \diag(d_1,...,d_p)$ be a diagonal matrix. We define {\em soft-thresholding} as
\begin{equation*}
D_\lambda = \diag((d_1 - \lambda)_+,(d_2 - \lambda)_+,...,(d_p - \lambda)_+),\label{eq:thresholding}
\end{equation*}
where $(x)_+ = \max(x, 0)$, and {\em hard-thresholding} as
\begin{equation*}
D_\lambda^H = \diag(d_1,d_2,...,d_q,0,...,0),\label{eq:thresholding}
\end{equation*}
where $q = \argmin_k(d_k < \lambda)$.

\subsection{Direct approach}\label{ss:direct-matrix}

The optimization problem \eqref{eq:direct-individual} of the direct approach described in Section \ref{ss:direct} can be approximated in the matrix completion setting. First, note that the bias introduced by the grid is negligible if the grid is sufficiently large. We have
\begin{align}
  \left|\tilde y_{i,j} - \bw_i'\bb(t_{i,j})\right| &= \left|y_{i,g(j)} - \bw_i'\bb(t_{i,j})\right|\nonumber\\
  &= \left| y_{i,g(j)} - \bw_i' (\bb (\tau_{g_i(j)}) + \bb (t_{i,j}) - \bb (\tau_{g_i(j)}))\right|\nonumber\\
  &\leq \left| y_{i,g(j)} - \bw_i' \bb(\tau_{g_i(j)})\right| + \left|\bw_i' (\bb(t_{i,j}) - \bb(\tau_{g_i(j)}))\right|\label{eq:grid-snap}
\end{align}
and by the continuity of the basis on a closed interval $[t_{\min},t_{\max}]$ the second element in \eqref{eq:grid-snap} can be arbitrarily small if $T \rightarrow \infty$. For the simplicity of notation in the reminder of this work we assume that all the points are observed on the grid $G$ of $T$ equidistributed points. 

Now, we rewrite the optimization problem \eqref{eq:direct-individual} as a matrix completion problem
\begin{align}
 \argmin_{\{\bw_i\}}\sum_{i=1}^N \sum_{j=1}^{n_i}\left|y_{i,g_i(j)} - \bw_i' \bb(\tau_{g_i(j)}))\right|^2 &= \argmin_{\{\bw_i\}}\sum_{(i,k) \in \Omega}\left|y_{i,k} - \bw_i' \bb(\tau_{k}))\right|^2\nonumber\\
%% \end{align*}
%% or, in the matrix notation
%% \begin{align*}
&= \argmin_W \| P_\Omega(Y - WB') \|_F^2,\label{eq:direct-matrix}
\end{align}
where by optimization over $\{\bw_i\}$ we mean optimization over all $\{\bw_i : i \in \{ 1,2,...,N \}\}$ and $W$ is an $N \times K$ matrix composed of vectors $\{\bw_i'\}$ stacked vertically.

The matrix formulation in equation \eqref{eq:direct-matrix} and the classic approach in Section \ref{ss:direct} share multiple characteristics. In both cases if data is dense, we may find an accurate representations of the underlying process simply by fitting least-squares estimates of $W$ or $\{\bw_i\}$. Conversely, if the data is too sparse, the problem becomes ill-posed and the least-squares estimates overfit. %Moreover, with $T \rightarrow \infty$ two frameworks become equivalent.

However, representations \eqref{eq:direct-individual} and \eqref{eq:direct-matrix} differ algebraically and this difference constitutes the foundation for the method introduced in the sequel of this paper. The matrix representation enables us to use the matrix completion framework and, in particular, in Section \ref{ss:matrix-factorization} we introduce convex optimization algorithms for solving \eqref{eq:direct-matrix}. % with additional constraints.

Note that some low-rank constraints on the random effect from the mixed-effect model introduced in Section \ref{ss:lmm} can be expressed in terms of constraints on $W$ and they can be potentially solved without imposing probability distributions. In particular in Section \ref{ss:low-rank-matrix}, as we show next, the linear mixed-effect model can be expressed by constraining $\rank(W)$. 
% for some $q \in \N$. 

\subsection{Low-rank matrix approximation}\label{ss:low-rank-matrix}
In the low-rank approach described in Section \ref{ss:reduced-rank} we assume that individual trajectories can be represented in a low-dimensional space, by constraining the rank of $W$.

We might attempt taking the same route for solving \eqref{eq:direct-matrix}. However, the problem with a constraint on the rank of $W$ is a non-convex problem. Motivated by results from matrix completion, we relax the rank constraint in \eqref{eq:direct-matrix} to a nuclear norm constraint and we attempt to solve

%% In many situations, this rank constrain can be efficiently relaxed by a nuclear norm to make the problem convex \citep{candes2009exact, fazel2002matrix}. \citet{candes2009near} consider the nuclear norm relaxation in the setting where the solution matches exactly a linear combination of the observed elements. In particular, they assume existence of exact solutions of \eqref{eq:direct-matrix} and they solve
%% \begin{align} 
%% \text{minimize\ \ } & \|W\|_*,  \nonumber\\
%% \text{subject to\ \ } & \| P_\Omega(Y - WB') \|_F^2 = 0.
%% \label{eq:cai}
%% \end{align}
%% The problem \eqref{eq:cai} can be solved using modern convex optimization software \citep{grant2008graph,boyd2004convex}, such as SDPT3 \citep{toh1999sdpt3} or SeDuMi \citep{sturm1999using}. However, with large matrices $Y$, these general algorithms can become computationally expensive and \citet{cai2010singular,mazumder2010spectral} introduced alternative iterative algorithms leveraging the low-rank and sparse decomposition.

%% For our application, the constraint \eqref{eq:cai} requiring exact matching of the observed and predicted values is too strong. For example, if there are more measurements per subject than $\rank(W)$ or if a subject was measured at two consecutive grid-points with some observation noise, we may not be able to find a continuous solution imposed by the smooth basis $B$.

\begin{align}
  \argmin_W \| P_\Omega(Y - WB') \|_F^2 + \lambda\|W\|_*,
%% \text{minimize\ \ } & \|W\|_*,  \nonumber\\
%% \text{subject to\ \ } & \| P_\Omega(Y - WB') \|_F^2 \leq \delta
\label{eq:rank-restricted}
\end{align}
for some parameter $\lambda > 0$.

\citet{cai2010singular} propose a first-order singular value thresholding algorithm (SVT), for solving a general class of problems involving a nuclear norm penalty and a linear form of $Y$, which includes \eqref{eq:rank-restricted}. Their algorithm can handle large datasets and has strong guarantees on convergence but it requires tuning the step size parameter, which can greatly influence the performance. This limitation was addressed by \citet{ma2011fixed,mazumder2010spectral,hastie2015matrix} who introduced iterative procedures which do not depend on such tuning. Moreover, \citet{hardt2014fast,chen2015fast} propose methods for the non-convex problem and \citet{ge2016matrix} argue that the non-convex problem has no spurious local minima.

In the last decade, the machine learning and statistical learning communities have introduced multiple algorithms for matrix completion. Many of them are suitable for solving \eqref{eq:rank-restricted}. However, in this article we focus on analyzing benefits of framing the trajectory prediction problem \eqref{eq:direct-individual} in the matrix completion framework, rather than on benchmarking existing solutions.

We argue that the matrix formulation \eqref{eq:rank-restricted} has two key advantages over the probabilistic approach (Section \ref{ss:reduced-rank}). First, the problem \eqref{eq:rank-restricted} does not impose any assumption on the distribution of $W$ or on the distribution of the noise. This property is particularly important whenever the data does not follow normal distribution. We further describe links between the matrix factorization and low-rank mixed-effect models in Section~\ref{s:the-link}. Second, the elementary formulation of \eqref{eq:rank-restricted} allows us to generalize this model to the multivariate or regression setting as we discuss in Section \ref{s:multivariate}.  

%In Section \ref{ss:matrix-factorization} we introduce methods for solving a relaxed version of this problem. % 

% Estimation of the {\it Karhunen-Lo\`eve} expansion becomes difficult when the data is sparsely observed.
% The functional principal components for sparse data, introduced by \citep{james2000principal}, is a parametric method that remedies problems of the direct approach. Similarly as in direct approach, it imposes continuity by assuming that observations come from underlying continuous processes, however, since the dimensionality of the basis might still be too large for estimation, the model further constrains the space to just a few principal components driving the variability of observed processes.

% In this model, partially observed curves $Y_i$ for $i \in \{1,2,...,N\}$ are assumed to come from a distribution
% \[
% Y_i \sim \cN(\mu, M \Sigma M' + \Gamma),
% \]
% where $I_T$ is an identity matrix, $\Sigma$ can be approximated by a low-rank matrix and $\Gamma_{T\times T}$ represents covariance of the noise. For the illustration of the idea, we assume $\Gamma = \sigma^2 I_T$, with some $\sigma > 0$ and an identity matrix $I_T$. By the spectral theorem, we decompose $\Sigma$ to $\Sigma = V \Lambda V'$ with a diagonal matrix $D$ and orthogonal $V$. We rewrite distribution of $Y_i$ as
% \begin{equation}\label{eq:probabilistic-model}
% Y_i \sim \cN(\mu, M V \Lambda V' M' + \sigma I_T),
% \end{equation}
% and we use maximum likelihood method to estimate $\sigma, V, \Lambda$. We are maximizing
% \[
% \prod_{i=1}^N \frac{1}{(2\pi)^{T/2} |M V \Lambda V' M' + \sigma^2I_T|^{1/2}} \exp\left\{ -(Y_i)'(M V \Lambda V'M' + \sigma^2 I_T )^{-1} (Y_i) / 2\right\}.
% \]
% Assume that we know the latent scores $\alpha_i$ and we need to find the projection space. Then the likelihood can be written as
% \[
% \prod_{i=1}^N \frac{1}{(2\pi)^{T/2} \sigma^T |\Lambda|^{1/2}} \exp\left\{ -(Y_i' - \alpha_i' V' M')'(Y_i' - \alpha_i' V' M') / 2\sigma^2 - \frac{1}{2}\alpha_i' \Lambda^{-1} \alpha_i \right\}.
% \]
% \citet{james2000principal} introduced an algorithm for estimating the mean $\mu$ and finding $(\alpha_i)$ and $(\sigma, V)$ iteratively by Expectation-Maximization (EM).
% % Maximizing this expression is equivalent \citep{james2000principal} to minimizing
% % \begin{align*}
% % \sum_{i=1}^N \left\{ (Y_i - \alpha_iV M)'(Y_i - \alpha_iV M) + \sigma^2 \sum_{j=1}^k \frac{\alpha_{i,j}^2}{D_{jj}}\right\} =&\\
% % \sum_{i=1}^N \| Y_i - \alpha_iV M\|^2 + \sigma^2 \sum_{j=1}^k \frac{1}{D_{jj}}\sum_{i=1}^N\alpha_{i,j}^2 =&\\
% % \| Y - AV M\|_F^2 + \sigma^2 \| A D^{-1/2} \|_F^2,
% % \end{align*}
% % where $A = [\alpha_1,\alpha_2,...,\alpha_N]'$.
% % Thus, the algorithm solves
% % \begin{align}\label{eq:optpca}
% % \argmin_{\sigma,A,V}\| Y M' - AV\|_F^2 + \sigma^2 \| A D^{-1/2} \|_F^2.
% % \end{align}

\subsection{Low-rank approximation with singular value thresholding}\label{ss:matrix-factorization}

The low-rank probabilistic approach, introduced in Section \ref{ss:reduced-rank}, is based on the observation that the underlying processes for each subject can be approximated in a low-dimensional space. %In $P_\Omega(Y)$, it is only feasible to estimate a low-rank representation of $Y$ matrix. %\citet{fazel2002matrix} show that, in order to make the optimization problem convex, the low-rank constraint \eqref{eq:rank-restricted} might be relaxed to a constraint on the nuclear norm of a matrix.
Here, we exploit the same characteristic using a matrix-factorization techniques for solving the optimization problem \eqref{eq:rank-restricted}.

%[...] Note, that in the Lagrangian form, the problem \eqref{eq:rank-restricted} becomes 
%\begin{align}\label{eq:mazumder}
%\min_{\tilde{W}} \frac{1}{2} \|P_\Omega(Y - \tilde{W})\|_F^2 + \lambda\|\tilde{W}\|_*,
%\end{align}
%for some $\lambda = \lambda(\sigma) \geq 0$ and $\tilde{W}$ is a $N \times T$ matrix. \citet{hastie2015matrix} introduced a fast and parallel algorithm for solving \eqref{eq:mazumder}.
% In our longitudinal setting, estimating $\tilde{W}$ directly from \eqref{eq:mazumder} will fail since it does not account for the continuous structure. Instead, we further constrain \eqref{eq:mazumder} by imposing the continuous basis $B$. We optimize
%% \begin{align}\label{eq:matrixproblem-final}
%% \min_Z \frac{1}{2} \|P_\Omega(Y - WB')\|_F^2 + \lambda\|W\|_*,
%% \end{align}
%% for some $\lambda > 0$ and where $W$ is an $N \times K$ matrix.

For the sake of clarity and simplicity, we choose to solve the problem \eqref{eq:rank-restricted} with an extended version of the \textsc{Soft-Impute} algorithm designed by \citet{hastie2015matrix,mazumder2010spectral}. As discussed in Section \ref{ss:low-rank-matrix}, many other convex optimization algorithms can be applied.

The key component to the solution is the following property linking problem \eqref{eq:rank-restricted} with the singular value decomposition (SVD). Consider the fully observed case of \eqref{eq:rank-restricted}. Using Theorem 2.1 in \citet{cai2010singular}, one can show that the optimization problem
\begin{align}\label{eq:optsvd}
\argmin_{W} \frac{1}{2} \| Y - WB' \|_F^2 + \lambda\|W\|_*
\end{align}
has a unique solution $W = \cS_\lambda (YB)$, where $\cS_\lambda(X) = UD_\lambda V'$ and $X = UDV'$ is the SVD of $X$. We refer to $\cS_\lambda(X)$ as the singular value thresholding (SVT) of $X$.

% \section{Longitudinal impute algorithms}
In order to solve \eqref{eq:optsvd} with a sparsely observed  $Y$, we modify the \textsc{Soft-Impute} algorithm  to account for the basis $B$. Our Algorithm \ref{alg:soft-impute} iteratively constructs better approximations of the global solution for each $\lambda$ in some predefined set $\{\lambda_1, \lambda_2, ..., \lambda_k\}$. For a given approximation of the solution $W^{old}$, we use $W^{old}B$ to impute unknown elements of $Y$ obtaining $\tilde{Y}$. Then, we construct the next approximation $W^{new}$ by computing SVT of $\tilde{Y}$.

%In Appendix \ref{s:convergence} we show that $W^{new,k}$ converges to the global solution $W_\lambda$ of \eqref{eq:matrixproblem-final}.

As a stopping criterion we compute the change between subsequent solution, relative to the magnitude of the solution, following the methodology in \cite{cai2010singular}. We set some fixed threshold $\varepsilon > 0$ for this criterion.

\begin{algorithm}
\caption{\textsc{Soft-Longitudinal-Impute}\label{alg:soft-impute}}
\begin{enumerate}
\item Initialize $W^{old}$ with zeros
\item Do for $\lambda_1 > \lambda_2 > ... > \lambda_k$:
\begin{enumerate}
\item Repeat:
\begin{enumerate}
\item Compute $W^{new} \leftarrow S_{\lambda_i}( (P_\Omega(Y) + P_\Omega^\perp(W^{old}B'))B )$
\item If $\frac{\|W^{new} - W^{old}\|_F^2}{\|W^{old}\|_F^2} < \varepsilon$ exit
\item Assign $W^{old} \leftarrow W^{new}$
\end{enumerate}
\item Assign $\hat{W}_{\lambda_i} \leftarrow W^{new}$
\end{enumerate}
\item Output $\hat{W}_{\lambda_1}, \hat{W}_{\lambda_2}, ... , \hat{W}_{\lambda_k}$
\end{enumerate}
\end{algorithm}

The common bottleneck of algorithms introduced by \citet{cai2010singular,mazumder2010spectral,ma2011fixed} as well as other SVT-based approaches is the computation of the SVD of large matrices. This is particularly severe in typical matrix completion settings, such as the Netflix problem, where the matrix size is over $400{,}000 \times 20{,}000$. However, in our problem,
\begin{align}\label{eq:small-rank}
  \rank(WB') \leq \rank(B) = K \ll N,
\end{align}
with $K \sim 10$ in our motivating data example. While algorithms for large matrices are applicable here, the property \eqref{eq:small-rank} makes the computation of SVD feasible in practice with generic packages such as PROPACK \citep{larsen2004propack}.

\paragraph{Modeling new data.}

For modeling new data, in practice, we are interested in two scenarios: (1) we collect new measurements of an existing patient $i \in \{1,2,...,N\}$; (2) we include a new patient $(N+1)$ with at least as many observations as the rank of $W$. In both cases, we simply use the current fit for each parameter $\lambda$ and update all models with newly observed data. This approach not only estimates new predictions but also marginally improves the existing model. 

%\tr{[TODO] If $n_i < q$, where $q$ is the rank of the current model, we might impose some additional conditions on $U_i$ and/or shrink it}

% \paragraph{Convergence.}

% Convergence of the algorithm can be proven by an elementary extension of the proofs in \citet{mazumder2010spectral}. We demonstrate a formal mapping of our problem to their setting Appendix \ref{s:convergence}.

\subsection{$\ell_0$-norm regularization}

While the nuclear norm relaxation \eqref{eq:rank-restricted} is motivated by making the problem convex, \citet{mazumder2010spectral} argue that in many cases it can also give better results than the rank constraint. They draw an analogy to the relation between best-subset regression ($\ell_0$ regularization) and LASSO ($\ell_1$ regularization as in \citet{tibshirani1996regression, friedman2001elements}). In LASSO by shrinking model parameters we allow more parameters to be included, what can potentially improve the prediction if the true subset is larger than the one derived through $\ell_0$ regularization. In the case of \eqref{eq:rank-restricted}, shrinking the nuclear norm allows us to include more dimensions of $W$ again potentially improving the prediction if the true dimension is high.

%While the nuclear norm normalization is motivated by the convex relaxation of the rank, \citet{mazumder2010spectral} observe that in many cases it can outperform rank restricted least squares. Analogically to \textsc{Lasso}, shrinkage can allow a rank higher than the actual rank of the matrix, improving prediction.

Conversely, the same phenomenon can also lead to problems if the underlying dimension is small. In such case, shrinking may allow including unnecessary dimensions emerging from noise. To remedy this issue, following the analogy with penalized linear regression, we may consider another classes of penalties. In particular, we may consider coming back to the rank constraint by modifying the nuclear norm penalty \eqref{eq:rank-restricted} to $\ell_0$. We define $\|W\|_0 = \rank(W)$. The problem
\begin{align}\label{eq:matrixproblem-final-l0}
\min_{W} \frac{1}{2} \|P_\Omega(Y - WB')\|_F^2 + \lambda\|W\|_0,
\end{align}
has a solution $W = \cS_\lambda^H (YB)$, where $\cS_\lambda^H(X) = UD_\lambda^H V'$ and $X = UDV'$ is the SVD of $X$. We refer to $\cS_\lambda^H(X)$ as the hard singular value thresholding (hard SVT) of $X$. We use Algorithm \ref{alg:hard-impute} to find the hard SVT of $YB$.


% refering to \citet{mazumder2010spectral}. %Multiple extensions and analogies to regularization in linear models can be drawn in this setting. See \citep{mazumder2010spectral} for more details on possible extensions of the problem \eqref{eq:rank-restricted}.

\begin{algorithm}
\caption{\textsc{Hard-Longitudinal-Impute}\label{alg:hard-impute}}
\begin{enumerate}
\item Initialize $W^{old}_{\lambda_i}$ with solutions $\tilde{W}_{\lambda_i}$ from \textsc{Soft-Longitudinal-Impute}
\item Do for $\lambda_1 > \lambda_2 > ... > \lambda_k$:
\begin{enumerate}
\item Repeat:
\begin{enumerate}
\item Compute $W^{new} \leftarrow S_{\lambda_i}^H( (P_\Omega(Y) + P_\Omega^\perp(W^{old}B'))B )$
\item If $\frac{\|W^{new} - W^{old}\|_F^2}{\|W^{old}\|_F^2} < \varepsilon$ exit
\item Assign $W^{old} \leftarrow W^{new}$
\end{enumerate}
\item Assign $\hat{W}_{\lambda_i} \leftarrow W^{new}$
\end{enumerate}
\item Output $\hat{W}_{\lambda_1}, \hat{W}_{\lambda_2}, ... , \hat{W}_{\lambda_k}$
\end{enumerate}
\end{algorithm}

The problem \eqref{eq:matrixproblem-final-l0} is non-convex, however, by starting from the solutions of Algorithm \ref{alg:soft-impute} we explore the space around the global minimum of the $\ell_1$ version of the problem. This strategy has been shown successful empirically \citep{ge2016matrix}.

% and in recent years there has been a growing interest in analyzing properties of non-convex algorithms for matrix completion \cite{sun2016guaranteed}.

\subsection{The link between reduced-rank model and \textsc{Soft-Longitudinal-Impute}}\label{s:the-link}

%Let $L < K$ be a number of true factors and let $\Tau_{L\times K}$ be a matrix of $L$ factor loadings in the basis $M_{K\times T}$.

Intuitively we might expect similarity between the principal directions derived using the probabilistic approach \eqref{eq:probabilistic} and their counterparts derived from the SVT-based approach. We investigate this relation by analyzing behaviour of SVT for matrices sampled from the probabilistic model given by \eqref{eq:probabilistic}.

For simplicity, let us assume that $\mu = 0$ and the data is fully observed on a grid $G$ of $T$ time-points. Assume that observations $i \in \{1,2,...,N\}$ come from the mixed-effect model
\begin{align}
  \by_i | \bw_i &\sim \cN( B \bw_i, \sigma^2 I_T), \label{eq:model-y}\\
  \bw_i &\sim \cN(0 , \Sigma), \nonumber%\label{eq:model-w}
\end{align}
% V \Lambda V'
%
where $\Sigma$ is an unknown covariance matrix of rank $q < K$ and variables $\{\bw_i, \by_i\}$ are independent. By the spectral decomposition theorem we decompose $\Sigma = V\Lambda V'$, where $V$ is a $K\times K$ orthogonal and $\Lambda$ is a diagonal $K\times K$ matrix with $q$ positive coefficients in decreasing order.
Since $\by_i$ and $\bw_i$ are independent, the distribution \eqref{eq:model-y} can be rewritten as
\begin{align}
  \by_i &\sim \cN(0, B V \Lambda V'  B' + \sigma^2 I_T). \label{eq:model}
\end{align}
Note that, the model \eqref{eq:model} is a factor model with $q$ factors---the first $q$ columns of $BV$.

The following theorem constitutes a link between the mixed-effect model and SVT. It is adapted from Theorem 9.4.1 in \citet{mardia1980multivariate}, derived from \citet{joreskog1967some}, 

\begin{theorem}\label{thm:maxlike}
  Let $Y = [\by_1,...,\by_N]'$ be the observed matrix and let $S_{\sigma^2}(YB) = UD_{\sigma^2}Q'$. Then, $(D_{\sigma^2},Q)$ is the maximum likelihood estimator of $(\Lambda,V)$.
\end{theorem}

 %One approach to estimate parameters in \eqref{eq:model-y} is to first decompose $YB = U D W'$ using SVD. ,
Factor analysis methods give not only estimates of $\Lambda$ and $V$ but also estimates of the individual latent variables $W = [\bw_1,...,\bw_N]'$.
%% For a fixed $\sigma$, the likelihood wrt. $(\lambda,V)$ is maximized by
%% \[
%%  \Lambda = D_{\sigma^2} \text{ and } V = Q,
%% \]
%% where $D_\sigma$ is the thresholded $D$ as defined in \eqref{eq:thresholding}.
In the multivariate analysis literature, there are multiple ways to estimate factor scores, i.e. a matrix $A$ such that $X \sim AD_{\sigma^2}V'$, most notably Spearman's scores, Bartlett's scores, and Thompson's scores \citep{kim1978factor}. %\citet{james2000principal} find maximum likelihood estimates for all parameters together.
Simply taking $W = U$ as the estimate of the scores corresponds to the solution of \eqref{eq:optsvd} as long as $\lambda = \sigma^2$.

Note that in Theorem \ref{thm:maxlike} we assume that $\sigma^2$ is known, which is infeasible in practice. However, the likelihood of $(V,\sigma)$ can be parametrized by $\sigma$ and we can find the optimal solution analytically. This corresponds to minimizing \eqref{eq:optsvd} for different $\lambda$.

%In the sparsely observed case, we want to approximate the same 
%\tr{TODO} }

This intuition is confirmed in our simulation study in Section \ref{s:simulation} (see Figure \ref{fig:principal-components}). Note that a similar analogy is drawn between the classical principal component analysis and probabilistic principal component analysis by \citet{tipping1999probabilistic} and \citet{james2000principal}. 

% Bartlett's estimate of the scores is
% \[
% U' = (\Theta' \Psi^{-1} \Theta)^{-1} \Theta'\Psi^{-1} X',
% \]
% where in our case $\Psi = \sigma^2 I$. thus
% \begin{align*}
% U' &= (\Theta' \sigma^{-2} I \Theta)^{-1} \Theta'\sigma^{-2} I X'\\
% &= (D_{\sigma^2}^{1/2}V' \sigma^{-2} VD_{\sigma^2}^{1/2})^{-1} D_{\sigma^2}^{1/2}V' \sigma^{-2} X'\\
% &= \sigma^{2}(D_{\sigma^2}^{1/2} D_{\sigma^2}^{1/2})^{-1} D_{\sigma^2}^{1/2}V' \sigma^{-2} X'\\
% &= (D_{\sigma^2})^{-1/2}V' X'.
% \end{align*}
% Bartlett's factor score is an unbiased estimator of factor scores.

% This method decomposes $Y$ to
% \begin{align*}
% X \sim U D_{\sigma^2}^{1/2} V',
% \end{align*}
% so, if we define $\tilde{U} = U D^{-1/2}$, we have
% \begin{align*}
% X \sim \tilde{U} D^{1/2}D_{\sigma^2}^{1/2} V',
% \end{align*}
% which implies that the factor model solves \eqref{eq:optsvd} with $\lambda = \sigma^2$.

% \paragraph{Thompson's factor scores}
% Thompson's estimate of the scores is
% \[
% U' = (I + \Theta' \Psi^{-1} \Theta)^{-1} \Theta'\Psi^{-1} Y',
% \]
% which gives
% \begin{align*}
% U' &= (I + D_{\sigma^2})^{-1} D_{\sigma^2}^{1/2}V' Y'
% \end{align*}

% It gives more accurate predictions \citet{paterek2007improving}.

% \citet{krzanowski1995multivariate} and \citet{tipping1999probabilistic} show that
% \[
% \sigma_{ML}^2 = \frac{1}{p-q} \sum_{i = p-q+1}^d\lambda_i.
% \]
% Other references: \citet{rubin1982algorithms}.



% \subsection{Factor analysis (PC)}

% We initially 'guess' an estimator for $\sigma$, say $\hat\sigma_1$ in \eqref{model:x}. We decompose $S~-~\hat\sigma_1^2I_d~=~\hat\Theta_1 \hat\Theta_1'$. We estimate $\sigma$ and estimate $\hat\Theta_2$. We iterate and converge to $\hat\Theta$. See \citet{lawley1971factor}.

% \subsection{Factor analysis (centroid method)}
%  \citet{lawley1971factor}

%\subsection{Tresholded SVD}
%Let $X_1,X_2,...,X_n$ be observations from \eqref{model:x}. Let $\bX = [X_1',X_2',...,X_n']'$. Consider

\section{Multivariate longitudinal data}\label{s:multivariate}

In practice, we are often interested in prediction of a univariate process in the context of other longitudinal measurements and covariates constant over time. Examples include prediction of disease progression given patient's demographics, data from clinical visits at which multiple blood tests or physical exams are taken, or measurements which are intrinsically multivariate such as gene expression or x-rays collected over time. The growing interest in this setting stimulated research in latent models \citep{sammel1996latent} and multivariate longitudinal regression \citep{gray1998estimating,gray2000multidimensional}. \citet{diggle2002analysis} present an example case study in which longitudinal multivariate modeling enables estimation of joint confidence region of multiple parameters changing over time, shrinking the individual confidence intervals.

In this section, we present an extension of our univariate framework to multivariate measurements sparse in time. We explore two cases: (1) dimensionality reduction, where we project sparsely sampled multivariate processes to a small latent space, and (2) linear regression, where we use a multivariate process and covariates constant over time for prediction of a univariate process of interest. To motivate our approach, we start with a regression with two variables observed over time.

%Both cases are directly motivated by medical applications where multiple clinical tests are taken sparsely in time. The dimensionality reduction allows comparing subjects with each other in a smaller space, independent of the sampling time of the original processes. The regression setting allows using additional covariates in the estimation of the progression of the parameter of interest.  

\subsection{Motivation: Univariate regression}

%% Suppose we observe two processes defined as in \eqref{eq:latent-probabilistic} and \eqref{eq:probabilistic}. Let $B$ be an orthogonalized $T\times K$ matrix of $K$ splines evaluated on a grid $G$ of $T$ timepoints and define
%% \begin{align}\label{eq:latent-probabilistic-bivar}
%%   \mathbf{x|u} \sim \cN(\mu_x + B\mathbf{u}, \sigma_x^2I_{T})  \text{ \ and \ }
%%  \mathbf{y|w} \sim \cN(\mu_y + B\mathbf{w}, \sigma_y^2I_{T}),
%% \end{align}
%% where
%% \begin{align}
%%  \mathbf{u} \sim \cN(0, \Sigma_u)  \text{ \ and \ } \mathbf{w} \sim \cN(0, \Sigma_w),
%% \end{align}
%% $\Sigma_u, \Sigma_w$ are $K \times K$ covariance matrices and we model individual observations as
%% Assume that $\mathbf{x}$ and $\mathbf{y}$ are linked by a linear model in the latent spaces, i.e.
%% \begin{align}\label{eq:latent-linear}
%%  \mathbf{w} = \mathbf{u}A + \varepsilon,
%% \end{align}
%% where $A$ is a $K \times K$ matrix and $\varepsilon \sim \cN(0,\Sigma_\varepsilon)$ for some covariance matrix $\Sigma_\varepsilon$. We want to estimate $\alpha$ and mutually improve estimates of $\bw$ and $\bu$ given the relation \eqref{eq:latent-linear}.

%% \tr{TODO: Comments on how to do it directly - the probabilistic approach}

Suppose, as previously, that the true processes are in a low-dimensional space of some continous functions (e.g. splines) and that we observe them with noise. More precisely, let
\begin{align}\label{eq:definitions-xy}
  \bx_i = B\bw_i + \be_{x,i} \text{\ \ and\ \ } \by_i = B\bu_i + \be_{y,i},
\end{align}
for $1 \leq i \leq N$, where $\bx_i,\by_i,\be_{x,i},\be_{y,i}$ are $T \times 1$ vectors, $\bw_i, \bu_i$ are $K \times 1$ vectors and $B$ is a $T \times K$ matrix of $K$ splines evaluated on a grid of $T$ points. We assume zero-mean independent errors $\be_{x,i},\be_{y,i}$ with fixed covariance matrices $\Sigma_X,\Sigma_Y$ respectively, and that the true processes underlying the observed $\bx_i$ and $\by_i$ follow a linear relation in the spline space, i.e.
\begin{align}\label{eq:linear-xy}
  \bu_i = A'\bw_i,
\end{align}
where $A$ is an unknown $K \times K$ matrix.

Let $X,Y,U,W$ be matrices formed from $\bx_i',\by_i',\bw_i',\bu_i'$ stacked vertically. From \eqref{eq:linear-xy} we have $U = WA$, while \eqref{eq:definitions-xy} implies
\begin{align}\label{eq:almost-errors-in-variables}
X = WB' + E_X \text{ \ and \ } Y = WA B' + E_{Y},
\end{align}
where $E_X,E_Y$ are matrices of observation noise. Without loss of generality we assume that $B$ is orthonormal. We have freedom to choose the basis $B$ and any basis can be orthogonalized using, for example, singular value decomposition. 

Note that by multiplying both expressions in \eqref{eq:almost-errors-in-variables} by $B$ and by orthogonality of $B$ we can map the problem to the classical multivariate {\em errors-in-variables models}. Let
\begin{align}\label{eq:errors-in-variables}
  \tilde{X} = XB = W + \tilde{E}_X \text{ and } \tilde{Y} = YB = WA + \tilde{E}_Y,
\end{align}  
where $\tilde{E}_X = E_XB$ and $\tilde{E}_Y = E_YB$. In errors-in-variables models it is assumed that the parameters $W$ and $A$ are unknown, and are to be estimated. Both regressors and responses are measured with noise (here $\tilde{E}_X$ and $\tilde{E}_Y$). The parameter $W$ can be interpreted as a latent representation of both $\tilde{X}$ and $\tilde{Y}$.

The problem of estimating parameters in \eqref{eq:errors-in-variables} has been extensively studied in literature dating back to \citet{adcock1878problem}. Two main methods for estimating parameters in \eqref{eq:errors-in-variables} are {\em maximum likelihood estimates (MLE)} and {\em generalized least squares estimators (GLSE)}. The estimators in MLE are derived under the assumption of certain ditributions of the errors. In GLSE, the only assumption on errors is that they are independent, zero-mean, and they have a common covariance matrix. Then, $\tilde{X} - W$ and $\tilde{Y} - WA$ are zero-mean and estimates for $W$ and $B$ can be found by optimizing some norm of these expressions. \citet{gleser1973estimation} show that in the no-interecept model for $\tilde{X}$ and $\tilde{Y}$ of the same size (as in our case) and under the assumption of gaussian errors, MLE and GLSE give the same estimates of $A$, if GLSE are derived for the Frobenius norm.

% We assume $\rank(U) = k$, where $k < K$. 

In this work we focus on the GLSE approach as it can be directly solved in our matrix factorization framework and we find it easier to deploy and extend in practice. %\cite{gleser1981estimation} shows that if variances of the noise are known, we can transform the data in such way that the variance matrices of the noise in variables are of the form $\Sigma_Y = \sigma^2_XI_T$ and $\Sigma_Y = \sigma^2_YI_T$.
To account for different magnitudes of the noise in $X$ and $Y$, we consider the optimization problem with weights
\begin{align}\label{eq:lag:bivariate}
  \minimize_{A,W} & \frac{1}{\sigma_X^2}\|XB - W \|_F^2 + \frac{1}{\sigma_Y^2} \| YB - WA \|_F^2,
%  \subjectto & \rank(W) = k.
\end{align}
where $\sigma_X,\sigma_Y > 0$. Let $\gamma = \sigma_X^2 / \sigma_Y^2$. Then \eqref{eq:lag:bivariate} can be transformed to
%% \begin{align}\label{eq:lag:bivariate2}
%%   \minimize_{A,W} & \| (\gamma Y:X) - W(\gamma AB':B') \|_F^2.%,\nonumber\\
%% %  \subjectto & \rank(W) = k.
%% \end{align}
%% Then \eqref{eq:lag:bivariate2} is equivalent to 
\begin{align}\label{eq:lag:bivariate3}
  \minimize_{A,W} & \| (XB : \gamma YB) - W(I: \gamma A) \|_F^2.%,\nonumber\\
%  \subjectto & \rank(W) = k.
\end{align}
To solve \eqref{eq:lag:bivariate3}, we show that the SVD of $(XB : \gamma YB)$ truncated to the first $K$ dimensions, can be decomposed to $W(I: \gamma A)$. Let $USV'$ be the SVD of $(XB : \gamma YB)$, with
\[
U = \begin{bmatrix}
  U_1 & U_2
\end{bmatrix},
S = \begin{bmatrix}
      S_{11} & S_{12}\\
      S_{21} & S_{22}
\end{bmatrix} 
\text{ and }
V = \begin{bmatrix}
      V_{11} & V_{12}\\
      V_{21} & V_{22}
    \end{bmatrix},
\]
where each $S_{ij}$ and $V_{ij}$ is a $K\times K$ matrix for $1 \leq i,j \leq 2$ and each $U_i$ is a $N \times K$ matrix for $1 \leq i \leq 2$. By Lemma 2.1 and Lemma 2.2 in \citep{gleser1981estimation} matrix $V_{11}$ is almost surely nonsingular. Therefore, $V_{11}^{-1}$ exists and we can transform the decomposition such that $(I : \gamma A) = (V_{11}')^{-1}\begin{bmatrix}V_{11}' & V_{21}'\end{bmatrix}$ and $W = U_1 S_{11} V_{11}'$, solving \eqref{eq:lag:bivariate3}.

%% Although we motivate the problem \eqref{eq:lag:bivariate2} as a regression of $Y$ on $X$, the two variables are symmetric. Therefore, the solution $\tilde{X}$ can be interpreted as a latent representation of the two processes. To emphasize this, we introduce a $N \times K$ latent matrix $W$, and we rewrite the problem as
%% \begin{align}\label{eq:lag:bivariate2}
%%   \minimize_{W,A_X,A_Y} & \| (Y:\gamma X) - (WA_Y'B':WA_X'B') \|_F^2,\nonumber\\
%%   \subjectto & \rank(W) = k.
%% \end{align}
%% As in \eqref{eq:rank-restricted}, the rank constrain can be relaxed to the nuclear norm. Then, Lagrangian takes form
%% \begin{align}
%%   \argmin_{W,A_X,A_Y} \| (Y:\gamma X) - (WA_Y'B':WA_X'B') \|_F^2 + \lambda \| W \|_*  \label{eq:final-bivariate},
%% \end{align}
%% and we apply methodology from Section \ref{ss:matrix-factorization} for finding $W,A_X$ and $A_Y$.

%% The low-rank matrix $W$ is a joint low-rank representation of matrices $X$ and $Y$, and thus our method can be seen as a dimensionality reduction technique. In Section \ref{ss:dim-red} we extended this idea to a larger number of variables. In Section \ref{ss:regression} we discuss how this approach can be used for regression.

%% Note that the approach presented in this section has an analogy in the matrix completion context. \citet{condli1999bayesian} showed that for improving prediction of unknown entries in some matrix $G$ one might consider a low-rank decomposition of $(G:H)$, where $H$ are additional covariates for each row, e.g., demographic features of individuals. The idea of stacking the matrices together motivates the dimensionality reductio ntechnique discussed in Section~\ref{ss:dim-red}.
% Therefore, the solution $U$ can be interpreted as a latent representation of the two processes. %% To emphasize this, we rewrite the problem as
%% \begin{align}\label{eq:lag:bivariate2-symmetric}
%%   \minimize_{U,A_X,A_Y} & \| (Y:\gamma X) - U(A_Y'B':A_X'B') \|_F^2.\nonumber
%% \end{align}
%% As in \eqref{eq:rank-restricted}, the rank constrain can be relaxed to the nuclear norm. Then, Lagrangian takes form
%% \begin{align}
%%   \argmin_{U,A_X,A_Y} \| (Y:\gamma X) - U(A_Y'B':A_X'B') \|_F^2 + \lambda \| U \|_*  \label{eq:final-bivariate},
%% \end{align}
%% and we apply methodology from Section \ref{ss:matrix-factorization} for finding $U,A_X$ and $A_Y$.

For partially observed data, if they are very sparse, it might be essential to constrain the rank of the solution. The partially-observed and rank-constained version of the problem \eqref{eq:lag:bivariate3} takes form 
\begin{align*}%\label{eq:lag:bivariate2-partial}
  \minimize_{A,W} & \| P_{\tilde\Omega}((X:\gamma Y) - W(B':\gamma A B')) \|_F^2,\nonumber\\
  \subjectto & \rank(W(B':\gamma A B')) = k,
\end{align*}
where $k$ is the desired rank of the solution and $P_{\tilde\Omega}$ is a projection on \[\tilde\Omega = \{(q,r): (q,r) \in \Omega \text{ or } (q,r-T) \in \Omega\}. \] As previously, for an unknown $k$ we can introduce a rank penalty using the nuclear norm
\begin{align}\label{eq:lag:bivariate2-partial}
  \minimize_{A,W} & \| P_{\tilde\Omega}((X:\gamma Y) - W(B':\gamma A B')) \|_F^2 + \lambda\|W(B':\gamma A B')\|_*.
\end{align}
The algorithm in the general case of multiple processes is derived in Section \ref{ss:dim-red}.

Although we motivate the problem as a regression of $Y$ on $X$, note that $X$ and $Y$ are symmetric in \eqref{eq:lag:bivariate3}. The low-rank matrix $W$ is therefore a joint low-rank representation of matrices $X$ and $Y$ and thus our method can be seen as a dimensionality reduction technique or as a latent space model. In Section \ref{ss:dim-red} we extended this idea to a larger number of variables. In Section \ref{ss:regression} we discuss how this approach can be used for regression.

The linear structure of \eqref{eq:almost-errors-in-variables} allows us to draw analogy not only to the errors-in-variables models but also to vast literature on {\em canonical corroletaion analysis (CCA)}, {\em partial least squares (PLS)}, {\em factor analysis (FA)}, and {\em linear functional equation (LFE) models}. \cite{borga1997unified} show that solutions of CCA and PLS can also be derived from the SVD of stacked matrices, as we did with $(XB:\gamma YB)$ in \eqref{eq:lag:bivariate3}. \cite{gleser1981estimation} thoroughly discusses the relation between errors-in-variables, FA and LFE.

%One can show that solving the problem \eqref{eq:lag:bivariate} is equivalent to finding the first $k$ principal directions in the canonical correlation analysis (CCA) \citep{hotelling1936relations}. Moreover, if $\gamma=1$ the problem is equivalent to PLS \citep{wold1975soft}.

Finally, note that the method of using SVD for stacked matrices has also been diretly applied in the recommender systems context. \citet{condli1999bayesian} showed that for improving prediction of unknown entries in some partially observed matrix $Q$ one might consider a low-rank decomposition of $(Q:R)$, where $R$ are additional covariates for each row, e.g. demographic features of individuals. %The idea of stacking the matrices together motivates the dimensionality reduction technique discussed in Section~\ref{ss:dim-red}.

%% Using SVD any solution can be written as $(\hat{X} : \hat{Y}) = W (V_X'M' : V_Y'M')$. Now let $\hat{X} = WV_X'M'$ and $\hat{Y} = WV_Y'M'$. 
%% The problem \eqref{eq:two-vars} can be becomes
%% \begin{align*}
%% \argmin_{W,V_X,V_Y} \frac{1}{2} \| XM - W V_X' \|_F^2 + \frac{1}{2} \| YM - W V_Y' \|_F^2 + \lambda\|W\|_*,
%% \end{align*}
%% which is equivalent to
%% \begin{align}\label{eq:final-bivar}
%% \argmin_{W,V_X,V_Y} \frac{1}{2} \| (XM:YM) - W (V_X:V_Y)' \|_F^2 + \lambda\|W\|_*.
%% \end{align}
%% where $(\cdot:\cdot)$ operator is concatenation of matrices column-wise. This can be solved using direct extension of Algorithm \ref{alg:soft-impute}.

\subsection{Dimensionality reduction}\label{ss:dim-red}

Suppose that for every individual we observe multiple variables varying in time (e.g. results of multiple medical tests at different times in a clinic) and we want to find a projection on $\R^d$ maximizing the variance explained for some $d \in \N$. This would correspond to characterizing patients by their progression trends.

%For fully observed $X_1,...,X_p$ this would correspond to the reduced-rank SVD of concatenated matrices $X_1~:~X_2~:~...~:~X_p$. %In this section, we introduce a method for finding such projection when the data is sparsely observed. We illustrate the method on bivariate processes, however it can be easily extended to any $p$-dimensional processes. 

We extend the equation \eqref{eq:lag:bivariate3} to account for a larger number of covariates and we do not impose decomposition of the solution yet. We formulate the following optimization problem
\begin{align*}%\label{eq:multivar}
  \argmin_{W} &\| (X_1B:X_2B:...:X_pB) - W \|_F^2 + \lambda\|W\|_*,
\end{align*}
where $X_i$ are some $N \times T$ matrices corresponding to the processes measured on a grid of $T$ points, $B$ is a basis evaluated on the same grid and orthogonalized (a $T \times K$ matrix), and $W$ is a $N \times pK$ matrix.

%where $W$ is a $N \times d$ matrix, $A = (A_1:A_2:...:A_p)$ with each $A_i$ is $d \times d_i$ for some $d_i \geq 1$ and $B_{i}$ is a basis selected for approximation of processes $X_{i}$ for $1 \leq i \leq p$. Each basis $B_i$ is $d_i$ dimensional and it is evaluated on some time-grid $T_i$.

Let $ \bB = I_p \otimes B $ be the Kronecker product of $p \times p$ identity matrix and $B$, i.e. a $pT \times pK$ matrix with $B$ stacked $p$ times on the diagonal, and let $\bX = (X_{1}: X_{2}:...: X_{p})$. Note that $\bB$ is orthogonal and therefore results developped in Section \ref{ss:reduced-rank} apply here. In particular, singular value thresholding solves
\begin{align*}%\label{eq:multivar-scaled}
\argmin_{W} &\| \bX - W \bB' \|_F^2 + \lambda\|W\|_*
\end{align*}
and we can use Algorithm \ref{alg:soft-impute} for solving
\begin{align}\label{eq:multivar-partially}
\argmin_{W} &\| P_{\Omega}\left(\bX - W\bB\right) \|_F^2 + \lambda\|W\|_*,
\end{align}
where $P_{\Omega}$ is the projection on observed indices $\Omega$.

Note that \eqref{eq:multivar-partially} can be further extended. First, we can allow for different basis for each process. Let $B_{i}$ be a basis selected for approximation of processes $X_{i}$ for $1 \leq i \leq p$, where each basis $B_i$ is $d_i$ dimensional and it is evaluated on some time-grid $T_i$. In particular, we allow $B_i = [1]$, which corresponds to a covariate $X_i$ constant in time for a given individual (e.g. demographics). Second, if measurements are on different scales, we may normalize them using scaling factors $\gamma_1, \gamma_2,..., \gamma_p > 0$. Then, the problem \eqref{eq:multivar-partially} takes form
\begin{align*}
\argmin_{Z} &\| P_{\Omega}\left((\gamma_1 X_{1}B_1:\gamma_2 X_{2}B_2:...:\gamma_p X_{p}B_p) - W\right) \|_F^2 + \lambda\|W\|_*,
\end{align*}
which we solve with the same techniques as \eqref{eq:multivar-partially}. Third, the observed indices $\Omega$ may vary in each process allowing for, for example, dense measurements of one process and sparse measurements of another one. 
% in the new basis $\bB = \diag[B_1,B_2,...,B_p]$.

%% See Algorithm \ref{alg:soft-PCA} for the detailed description of the procedure. Note that \textsc{Hard-Longitudinal-Impute} can be analogically extended to the multivariate setting. %Similarly, the proof of convergence follows from the proof in Section \ref{s:convergence}.

%% \begin{algorithm}
%% \caption{\textsc{Soft-Longitudinal-PCA}\label{alg:soft-PCA}}
%% \begin{enumerate}
%% \item Normalize $X_1,...,X_p$ (optional)
%% \item Initialize $W^{old},A^{old}_1,...,A^{old}_p$ with zeros
%% \item Do for $\lambda_1 > \lambda_2 > ... > \lambda_k$:
%% \begin{enumerate}
%% \item Repeat:
%% \begin{enumerate}
%% \item Compute $W^{prj}_i = (P_\Omega(X_i) + P_\Omega^\perp(W_i^{old}B'))B$ for $i \in \{1,...,p\}$,
%% \item Compute $W^{new} \leftarrow S_{\lambda_k}( W^{prj}_1 : ... : W^{prj}_p )$.
%% \item If $\frac{\|W^{new} - W^{old}\|_F^2}{\|W^{old}\|_F^2} < \varepsilon$ exit.
%% \item Assign $W^{old} \leftarrow W^{new}$
%% \end{enumerate}
%% \item Assign $\hat{W}_{\lambda_k} \leftarrow W^{new}$
%% \end{enumerate}
%% \item Output $\hat{W}_{\lambda_1}, \hat{W}_{\lambda_2}, ... , \hat{W}_{\lambda_k}$.
%% \end{enumerate}
%% \end{algorithm}

\subsection{Regression}\label{ss:regression}

In practice, we are interested in estimating progression of an individual parameter (e.g., cancer growth) given some individual features constant over time (e.g. demographics) or given progressions of other covariates (e.g. blood tests, vitals, biopsy results).

We start with a regression problem with fixed covariates and sparsely observed response trajectories. Let $X$ be a $N \times d$ matrix of observed covariates, $Y$ be a sparsely observed $N \times T$ matrix of trajectories, and $B$ be a $T \times K$ matrix representing a basis of $K$ splines evaluated on a grid of $T$ points. We consider the optimization problem
\begin{align}\label{regression}
 \argmin_A \| P_\Omega(Y - XAB')\|^2,
\end{align}
where $A$ is a $d \times K$ matrix and $P_\Omega$ is a projection on the observed indices $\Omega$. To solve \eqref{regression} we propose an iterative Algorithm \ref{alg:longitudinal-regression}.

\begin{algorithm}
\caption{\textsc{Sparse-Regression}\label{alg:longitudinal-regression}}
\vspace{3pt}
\begin{enumerate}
\item Initialize $A$ zeros
\item Repeat till convergence:
\begin{enumerate}
\item Impute regressed values $\hat{Y} = P_\Omega(Y) + P_\Omega^\perp(XAB')$
\item Compute $A^{new} \leftarrow (X'X)^{-1} X'\hat{Y}B$
\item If $\frac{\|A^{new} - A\|_F^2}{\|A\|_F^2} < \varepsilon$ exit
\item Assign $A \leftarrow A^{new}$
\end{enumerate}
\item Return $A$
\end{enumerate}
\end{algorithm}

% We suggest a two-step procedure. We first find a joint low-rank decomposition $W$ of covariates $X_1,...,X_p$. Now, if all variability is driven by covariates $W$ w

Suppose we want to incorporate other variables varying in time for prediction of the response process. We can directly apply the method proposed in Section \ref{ss:dim-red} and model the response and regressors together. However, it might be suboptimal for prediction, as it optimizes for least-squares distance in all variables rather than only the response. This difference is analogical to the difference between regression line of some univariate $y$ on independent variables $x_1,...,x_p$ and the first principal component analysis of $(y,x_1,...,x_p)$, used for prediction of $y$. While the first one minimizes the distance to $y$, the latter minimizes the distance to $(y,x_1,...,x_p)$, which is usually less efficient for predicting $y$.

Alternatively, we can use methodology from Section \ref{ss:dim-red} only for covariates. The solution of \eqref{eq:multivar-partially} can be decomposed into $W = USV'$, and we regress $Y$ on $U$ as in \eqref{regression}.
%However, some part of the covariance of processes $Y$ observed in the data might not be explained by covariates $X$. 
%Note that the latent representation $W$ can be constructed either by applying sparse functional PCA, by \textsc{Soft-Longitudinal-Impute}, jointly by \textsc{Soft-Longitudinal-PCA} or by other low-rank representation of $(X_1,...,X_p)$.
%To incorporate additional trends of variability in $Y$ we introduce an additional unknown covariate $Z$. Let $Z$ be an unknown $N \times K$ matrix of rank $d_1$ and
Let $U$ be an $N \times d_2$ orthogonal matrix derived from $(X_1,...,X_p)$, where $d_2$ is the numbers of latent components. We search for a $d_2 \times K$ matrix $A$ solving
\begin{align}\label{eq:pcr}
\minimize_{A} \|P_\Omega(Y - UAB')\|_F^2 + \lambda\|A\|_*,
\end{align}
where $P_\Omega$ is a projection on a set of observed coefficients.
%For a known $A$ we can find $Z$ by applying methodology developped in Section \ref{ss:reduced-rank} to the partially observed matrix $(Y - UAB')$. For a given $UA$, the problem \eqref{eq:pcr} is a regression of $Y - UAB'$ on $Z$ for which we we can use Algorithm \ref{alg:longitudinal-regression}. Based on this observation
We propose a two-step iterative procedure (Algorithm~\ref{alg:sparse-regression}).

% where we fill in the missing values in $Y$ with zeros, multiply the expression by $B$, regress $YB$ on $WA'$, and use the new $A$ for filling in the missing values, till convergence. We refer to Algorithm \ref{alg:sparse-regression} for the full description of the procedure.


%% Note that $B$ can be estimated by minimizing 
%% \begin{align}\label{eq:regression:minimization}
%% \argmin_B \|P_\Omega((W'W)^{-1}W'Y - BM')\|_F^2,
%% \end{align}

%% \tr{TODO: How it works. Regularization. References.\\
%% It should be very simple to show that Algorithm \ref{alg:sparse-regression} defines a contraction operator with the fixed-point equal to the solution of \eqref{eq:regression:minimization}. \citep{bertsekas1999nonlinear}
%% }
\begin{algorithm}
\caption{\textsc{Sparse-Longitudinal-Regression}\label{alg:sparse-regression}}
\vspace{3pt}
\begin{flushleft}
\textbf{Step 1: Latent representation}
\end{flushleft}
\begin{enumerate}
\item For sparsely observed $\bX = (X_1,X_2,...,X_p)$ find latent scores $U$ (Algorithm \ref{alg:soft-impute})
%\item Define $W = W_1:W_2:...:W_p$
%\item Orthogonalize $W$
\end{enumerate}
\begin{flushleft}
\textbf{Step 2: Regression}
\end{flushleft}
\begin{enumerate}
\item For each $\lambda_1,\lambda_2,...,\lambda_k$
  \begin{itemize}
  \item Get $A_{\lambda_i}$ by solving the regression problem \eqref{eq:pcr} with $\textbf{Y},U,\lambda_i$ (Algorithm \ref{alg:longitudinal-regression})
  %% \item Repeat until convergence:
  %%   \begin{enumerate}
  %%   \item Find $Z^{new}$ by solving $\|P_\Omega( (Y - UA_0B') - ZB')\|_F^2 + \lambda_i\|Z\|_*$ (Algorithm \ref{alg:soft-impute})
  %%   \item Find $A^{new}$ by solving $\|P_\Omega( (Y - Z^{new}B') - UAB')\|_F^2$ (Algorithm \ref{alg:longitudinal-regression})
  %%   \item If $\frac{\|A^{new} - A_0\|_F^2}{\|A_0\|_F^2} + \frac{\|Z^{new} - Z_0\|_F^2}{\|Z_0\|_F^2} < \varepsilon$ exit
  %%   \item Assign $A_0 \leftarrow A^{new},Z_0 \leftarrow Z^{new}$
  %%   \end{enumerate}
  %% \item Assign $A_{\lambda_i} \leftarrow A_0, Z_{\lambda_i} \leftarrow Z_0$
  \end{itemize}
\item Return $A_{\lambda_1}, A_{\lambda_2}, ..., A_{\lambda_k}$
\end{enumerate}
\end{algorithm}


\section{Simulations}\label{s:simulation}

We illustrate properties of the multivariate longitudinal fitting in a simulation study. First, we generate curves with quickly decaying eigenvalues of covariance matrices. Then, we compare performance of the methods in terms of the variance explained by the predictions. %We use bimodal distribution to emphisize that

\paragraph{Data generation} Let $G$ be a grid of $T = 31$ equidistributed points and let $B$ be a basis of $K = 7$ spline functions evaluated on the grid $G$. %We generate observations from the model $\eqref{eq:model}$ with $p = 3$ true components. %We set $V\Lambda V'$ to a matrix with eigenvalues $\diag[1-\frac{1}{7},...,1 - \frac{p}{8},0,...,0]$.
We simulate three $N \times K$ matrices using the same procedure $\cG(r_1, r_2, K, N)$, where $r_1, r_2 \in \R_+^K$:
\begin{enumerate}
\item Define the procedure $\cM(r)$ generating symmetric matrices $K \times K$ for a given vector $r \in \R_+^K$:
  \begin{enumerate}
    \item[(i)] Simulate $K \times K$ matrix $R$
    \item[(ii)] Use SVD to decompose $S$ to $UDV'$
    \item[(iii)] Return $Q = V \diag[r] V' $, where $\diag[r]$ is a diagonal matrix with $r$ on the diagonal
  \end{enumerate}
\item Let $\Sigma_1 = \cM(r_1)$, $\Sigma_2 = \cM(r_2)$ and $\mu = \cMN(0, I_K)$, where $\cMN$ denotes the multivariate Gaussian distribution
\item Draw $N$ vectors $v_i$ according to the distribution
\begin{align*}
v_i \sim 
\begin{cases}
\cMN(2\mu, \Sigma_1) & \text{ if } 1 \leq i \leq N/3\\
\cMN(-\mu, \Sigma_2) & \text{ if } N/3 < i \leq N
\end{cases}
\end{align*}
\item Return a matrix with rows $[v_i]_{1 \leq i \leq N}$.
\end{enumerate}
Define \[ r_1 = [1, 0.4, 0.005, 0.1 \exp(-3), ..., 0.1 \exp(-K+1)] \]  and \[ r_2 = [1.3, 0.2, 0.005, 0.1 \exp(-3), ..., 0.1 \exp(-K+1)],\]
and let $X_1,X_2,Z$ be generated following the procedure $\cG(r_1,r_2,K,N)$ and let $Y = Z + X_1 + X_2$. We consider $X_1,X_2$ and $Y$ as coefficients in a spline space $B$. We derive corresponding functios by multiplying these matrices by $B'$, i.e. $X_{f,1} = X_1B'$, $X_{f,2} = X_2B'$ and $Y_f = YB'$. We set $N=100$.

We uniformly sample $10\%$ indices $\Omega \subset \{1,...,N\} \times \{1,...,T\}$, i.e. around $3$ points per curve on average. Each ebserved element of each matrix $X_{f,1}, X_{f,2}$ and $Y$ is drawn with Gaussian noise with mean $0$ and standard deviation $0.25$. The task is to recover $Y$ from sparse observed elements $\{Y_{i,j} : (i,j) \in \Omega\}$. 

\paragraph{Methods}

We compare \textsc{Soft-Longitudinal-Impute} (SLI) defined in Algorithm \ref{alg:soft-impute} with the fPCA procedure \citep{james2000principal}, implemented in \citet{peng2009geometric}. Both, SLI and fPCA require a set of basis functions. In both cases we use the same basis $B$ as in data generation process. In SLI we also need the tuning parameter $\lambda$, whilst in fPCA we need to choose the rank $R$. We use cross-validation to choose $\lambda$ and $R$ by optimizing for the prediction error on held-out (validation) observations.

We divide the observed coefficients into training ($81\%$), validation ($9\%$) and test ($10\%$) sets. We choose the best parameters of the three models on the validation set and then retrain on the entire training and validation sets combined. % Finally, since in the simulated setting we have access to the ground truth, we compare algorithms on the entire curves.
We compute the error of entire curves by taking mean squared Frobenius distance between $Y$ and estimated $\hat{Y}$, i.e.
\begin{align}\label{eq:err}
 MSE(\hat{Y}) = \frac{1}{T|S|} \sum_{i\in S} \|Y_i - \hat{Y}_i \|_F^2
\end{align}
 on the test set $S$.
 
 For illustration, we also present results of \textsc{Sparse-Longitudinal-Regression} (SLR), regressing $Y$ on $X_1$ and $X_2$. Note, however, that the SLR, unlike two other methods, uses additional information about $Y$ contained in $X_1$ and $X_2$. This comparison is only meant to validate our approach.

 We train the algorithms with all combinations of parameters: the number of splines $K \in \{2,3,...,d\}$, regularization parameter for SLI and SLR procedures $\lambda \in \{10, 15, 20, ..., 50\}$ and the rank for fPCA procedure $d \in \{2,3,4\}$. We define the grid of $T = 51$ points. We compare the three methods fPCA, SLI and SLR, in reference to the baseline {\it null model} which we define as a LOWESS estimate of the population mean curve.

 \begin{figure}[h!]
  \includegraphics[width=0.49\linewidth]{../plots/components-fpca}
  \includegraphics[width=0.49\linewidth]{../plots/components-fimp}
  \caption{The first three principal components derived using sparse functional PCA (left) and Soft-Longitudinal-Impute (right). The components are ordered as follows: 1st red solid curve, 2nd green dashed curve, 3rd blue dotted line. As expected, estimates of components are very similar.}
  \label{fig:principal-components}
\end{figure}

\paragraph{Results}

\begin{figure}[h!]
  \includegraphics[width=0.49\linewidth]{../plots/pred-mean}
  \includegraphics[width=0.49\linewidth]{../plots/pred-fpca}
  \includegraphics[width=0.49\linewidth]{../plots/pred-fimp}
  \includegraphics[width=0.49\linewidth]{../plots/pred-freg}
  \caption{Two example curves (black and red) for each of the four methods. Solid lines are the true curves and dashed lines are the predictions. }
  \label{fig:example-predictions}
\end{figure}

The SLI achieves performance similar to \citep{james2000principal}, as presented in Table \ref{tbl:simulations}. The SLR, having access to additional information about $Y$, outperforms other methods validating its correctness.

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & MSE & \% std \\ 
  \hline
  fPCA & 0.124 & 0.03 \\ 
  SLI & 0.121 & 0.03 \\ 
  SLR & 0.064 & 0.03 \\ 
   \hline
\end{tabular}
\caption{Average (across 10 trials) variance explained by (1) subject's mean of observed points (mean), (2) functional principal components (fPCA), (3) Sparse-Longitudinal-Impute (SLI) and (4) Sparse-Longitudinal-Regression (SLR) with extra data available for this procedure.}
\label{tbl:simulations}
\end{table}

In Figure \ref{fig:principal-components} we present the first components derived from both sparse functional PCA and SLI. In Figure \ref{fig:example-predictions} we present example predictions from all four methods. In Figure \ref{fig:estimated-rank}, we present the estimated rank and cross-validation error of one of the simulation runs.

\begin{figure}[h!]
  \includegraphics[width=0.49\linewidth]{../plots/error-of-lambda}
  \includegraphics[width=0.49\linewidth]{../plots/K-of-lambda}
  \caption{Estimated error of the solution (left) and the estimated rank of the solution (right) depending on the parameter $\lambda$.}
  \label{fig:estimated-rank}
\end{figure}


% \begin{figure}[h!]
%   \includegraphics[width=0.47\linewidth]{images/ours-smpl}
%   \includegraphics[width=0.47\linewidth]{images/fpca-smpl}\\
%   \includegraphics[width=0.47\linewidth]{images/ours}
%   \includegraphics[width=0.47\linewidth]{images/fpca}
%   \caption{Top: noisy samples, Bottom: real curves, Left: ours, Right: fPCA}
%   \label{fig:basis}
% \end{figure}

\section{Data study}

The target class of problems motivating this study is prediction of trajectories of disease progression of individual patients from sparse observations. In this section, we present how our methods are applied for understanding progression of neurological disorders leading to motor impairments and gait pathologies. First, we discuss how practitioners collect the data and use them to guide decision process. Next, we describe our dataset and present how our methodology can improve current workflows. Then, we present and discuss results.

In clinical gait analysis, at each visit movement of a child is recorded using optical motion capture. Optical motion capture is a data acquisition technique. It allows to estimate 3D positions of body parts using a set of cameras tracking markers positions on the subject's body. A set of at least three markers is placed at each analyzed body segment (e.g. hand, lower leg, upper arm), so that its 3D position and orientation can be identified uniquely. These data is then used to determine relative positions of body segments by computing the angle between the corresponding planes. Typically it is done using a biomechanical model for enforcing biomechanical constraints and improving accuracy.

Practitioners are usually concerned about movement pattern of the lower-limbs and nine joints: ankle, knee, hip in each leg and pelvis (Figure \ref{fig:joint-angles}). Eeach joint angle is measured in time. For making the curves comparable between the patients, usually the time dimension is normalized to the percentage of gait cycle, defined as the time between two foot strikes (Figure \ref{fig:joint-angles-in-time}).

While trajectories of joint angles are a piece of data commonly used by practitioners for taking decisions regarding treatment, their high-dimensional nature hinders their use as a quantitative metric of gait pathology or treatment outcome. This motivates development of univariate summary metrics of gait impairment, such as questionnaire-based metrics Gillette Functional Assessment Walking Scale (FAQ) \citep{gorton2011gillette}, Gross Motor Function Classification System (GMFCS) \citep{palisano2008content} and Functional Mobility Scale (FMS) \citep{graham2004functional}, or observational videa analysis scores such as Edinburgh Gait Score \citep{read2003edinburgh}.

One of the most widely adopted quantitative measurements of gait impairments in pediatrics is Gait Deviation Index (GDI) \citep{schwartz2008gait}. GDI is derived from joint angle trajectories and measures deviation of the first ten singular values from the population average of the normally developing population. GDI is normalized in such a way that $100$ corresponds to the mean value of normally developing children, with the standard deviation equal $10$. It is proven to be highly correlated with questionnaire-based methods. Thanks to its deterministic derivation from the motion capture measurements this method is considered very objective.

%\afterpage{\clearpage}
\begin{figure}[p]
  \includegraphics[width=\linewidth]{images/bodies-chart-paper-layout.png}
  \caption{Four joints measured in clinical gait analysis: pelvis, hip, knee, and ankle. Each joint can be measured in three planes: saggital plane (top row), frontal plate (middle row), and transverse plane (bottom row).}
    \label{fig:joint-angles}
%  \vskip 0.5cm
  \includegraphics[width=\linewidth]{images/kinematics.pdf}
  \caption{Recordings of joint angles during the gait cycle (fraction of the gait cycle on each X axis)}
    \label{fig:joint-angles-in-time}
\end{figure}

%These signals are then converted to trajectories of joint angles using biomechanical models. Finally, practitioners extract summary statistics of gait, which are then used for clinical decisions.

In medical practice, GDI has been adapted as a metric for diagnosing severity of impairment and it constitutes an important part of the clinical decision taking and evaluation of treatment outcomes. However, in order to correctly identify the surgery outcome, its crucial to understand the natural progression of GDI. In particular, a positive outcome of a surgery, might be negligible when compared to natural improvement during puberty. Similarly, a surgery maintaning the same level of GDI might be incorrectly classified as a failaure, if the decline in patient's function over time is not accounted for.

%Therefore, a model of its progression can  an essential baseline for analyzing effects of treatments -- an accurate prediction of a trajectory can help practitioners understand if an improvement after therapy is due to the treatment or purely a result of natural progression.

Methods introduced in this article can be used to approximate individual progressions of GDI. First, we present how prediction can be done solely based on patient's GDI history and histories of other patients. Next, using our regression procedure, we predict GDI trajectories using other sparsely observed covariates, namely O2 expenditure and walking speed.

\subsection{Materials and methods}

We analyze a dataset of Gillette Children's Hospital patients visiting the clinic between 1994 and 2014, mostly diagnosed with Cerebral Palsy, age ranging between 4 and 19 years. The dataset contains $84$ visits of $36$ patients without gait disorders and $6066$ visits of $2898$ patients with gait pathologies.

Motion capture data was collected at 120Hz and joint angles in time were extracted. These joint angles were then normalized in time to the gait cycle, resulting in curves as in Figure \ref{fig:joint-angles-in-time}. Points from these curves were then subsampled (51 equidistributed points). Given the data in this form, we computed GDI metric from each visit and each leg.

In the dataset we received, for each patient we observe their birthday and disease subtype. From each visit we observe following variables: patient ID, time of the visit, GDI of the left leg, GDI of the right leg, walking speed, and O2 expenditure. Other clinical variables that we received from Gillette Children's Hospital were not included in this study. Note that the walking speed is related to information we loose during normalization of the gait cycle in time. O2 expenditure is a measure of subject's energy expenditure during walking. Pathological gait is often energy inefficient and reduction of O2 expenditure is one of the objectives of treatments. Finally, GDI is computed for two legs while in many cases the neurological disorder affects only one limb. To simplify the analysis, we focus on the more impaired limb by analyzing minimum of left and right GDIs.

Our objective is to model individual progression curves. We test three methods: functional principal components (fPCA), \textsc{Soft-Longitudinal-Impute} (SLI) and \textsc{Sparse-Longitudinal-Regression} (SLR). We compare the results to the \emph{null model} -- the population mean (\verb|mean|). In SLR, we approximate GDI using latent variables of sparsely observed covariates \textit{O2 expenditure} and \textit{walking speed}, following the methodology from Section \ref{ss:regression}.

In our evaluation procedure, for the test set we randomly select $5\%$ of observations of patients who visited the clinic at least $4$ times. Then, we split the remaining $95\%$ of observations into a training and validation sets in $90-10$ proportion. We train the algorithms with the following combinations of parameters: the regularization parameter for SLI and SLR procedures $\lambda \in \{0, 0.1, 0.2, ..., 2.0\}$ and the rank for fPCA procedure $d \in \{2,3,4,...,K\}$. We define the grid of $T = 51$ points. We repeat the entire evaluation procedure $20$ times.

Let us denote the test set as $\Omega \subset \{1,2,...,N\} \times \{1,2,...,T\}$. We validate each model $M$ on held-out indices by computing the mean squared error as defined in \eqref{eq:err}. We select the parameters of each of the three methods using cross-validation, using the same validation set.

For the purpose of this study we created an \verb|R| package \verb|fcomplete|. The package contains implementations of algorithms \ref{alg:soft-impute}, \ref{alg:hard-impute}, \ref{alg:longitudinal-regression}, and \ref{alg:sparse-regression}, and helper functions for transforming the data, sampling training and test datasets, and plotting functions. For convenince, we also provided an interface for using the \verb|fpca| package implementing Sparse Functional Principal Components algorithms \citep{james2000principal,peng2009geometric}. The analysis was perform on a desktop PC with 32 GB RAM memmory and an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-6700K CPU @ 4.00GHz, operating on a Ubuntu 18.04 system with \verb|R| version 3.4.4.

\begin{table}[ht]
  \centering
\begin{tabular}{rrr}
  \hline
 & mean & sd \\ 
  \hline
  fPCA & 0.73 & 0.16\\
  SLI & 0.70 & 0.10\\
  SLR & 0.68 & 0.08 \\
   \hline
\end{tabular}
\caption{Distribution of cross-validated MSE of the three procedures: functional principal components (fPCA), Soft-Longitudinal-Impute (SLI), Soft-Longitudinal-Regression with two additional predictors: O2 expenditure and walking speed (SLR).}
\label{tbl:data-res}
\end{table}

\subsection{Results}\label{ss:results}

Compared to the null model, all three methods explain around $\sim 30\%$ of the variance. We present detailed results in Table \ref{tbl:data-res}. \textsc{Sparse-Longitudinal-Regression} had smaller mean and variance than two other methods indicating that two other variables. We conclude that O2 expanditure and walking speed provide additional information for prediction of GDI progression.

Both fPCA and \textsc{Sparse-Longitudinal-Impute} provide latent representations of patients' pregression curves which can potentially be interpreted. To this end we first analyze the singular value vectors from our SVD solution to which we reffer as principal components.

In the left plot in Figure \ref{fig:data-components} we plotted the first two estimated principal components. We found that the first component estimates the change between GDI before and after age of 20. The second component models changes around age of 10 and around age of 18. In the right plof in Figure \ref{fig:data-components}, by adding a principle component to the population mean curve, we illustrate how differences in the first component are reflected in the patients trajectory. By visual investigation of curves returned by our \textsc{Sparse-Longitudinal-Impute} and by fPCA we found similar trends in the first two components.

Since our SVD decomposition defines a low-rank representation of the progression trends, we can also use it to gain insights on progression in different groups of patients. We find differences between trends of progression for different subtypes of paralysis of patients. In Cerebral Palsy we devide paralysis into subtypes depending on which limbs are affected: monolegia (one leg), diplegia (two legs), hemiplegia (one side of the body), triplegia (three limbs), quadriplegia (four limbs). Hemiplegia is the most prevalent in our population and it might be devided depending on severity, from type I (weak muscles, drop foot) to type IV (severe spasticity).

\begin{figure}[h!]
  \includegraphics[width=0.49\linewidth]{images/data-components.pdf}
  \includegraphics[width=0.49\linewidth]{images/data-components-added.pdf}
\caption{Left: Trends of variability (principal compontents). Right: Effect of principal components on individual trajectories.}
    \label{fig:data-components}
  \includegraphics[width=0.9\linewidth]{images/paralysis-subtypes}
\caption{Progression trends is differnt subsets of diseases. Negative values of the score, such as most of the quadriplegic group, correspond to individual trends where the first component (the red curve Figure \ref{fig:data-components} left) is subtracted from the mean (the green curve in Figure \ref{fig:data-components} right). Positive values of the score, such as most of the hemiplegic group, correspond to individual trends where the first component is added (the red curve in Figure \ref{fig:data-components} right).}
    \label{fig:subtypes}
\end{figure}

\section{Discussion}

Results presented in Section \ref{ss:results} imply that our \textsc{Sparse-Longitudinal-Impute} and \textsc{Sparse-Longitudinal-Regression} methods can be successfully applied to understands trends of variability of disease progression. We show how to incorporate progressions of O2 expenditure and walking speed in the prediction of the pregression of GDI. We present how low-rank representation can be laveraged to gain insights about sybtypes of impairment.

While a large portion of variance remains unexplained, it is important to note that in practice the individual progression is not accounted for explicitely in the current decision-making process. Instead, practitioners only use the population-level characteristics of the dependence between age and impairment severity. Our model can greatly improve current practice.

Despite successful application, we identify limatations that could be potentially addressed in the extensions of our model. Here, we name three of them. First, the method is meant to capture natural continuous progression of GDI, while in practice there are many discrete events, such as surgeries that break continuity assumption and render the mean trajectories less interpretable. Second, our methodology does not address the ``cold start problem'', i.e. we do not provide tools for predictions with only one or zero observations. Third, we do not provide explicit equations for confidence bounds of predicted paramaters.

While these and other limitations can constrain applicability of the method in the current form, they can be addressed using existing techniques of matrix factorization. The focus of this paper is to introduce a computational framework rather than build a full solution for all cases. Elementary formulation of the optimization problem as well as the full-functional \verb|R| implementation of the methods can foster development of new tools using matrix factorization for logitudinal analysis and for mixed-effect models.

% \end{figure}

% \section{Other applications}
% Some other cases where the structure can be available.
% \begin{enumerate}
% \item Images?
% \item General applications of low-rank methods: clustering
% \end{enumerate}

% \section{Discussion}
\renewcommand*{\bibfont}{\small}
\bibliography{report}
%\printbibliography

\appendix

\section{Proofs}\label{s:convergence}

We prove convergence by mapping our problem into the framework of \citet{mazumder2010spectral}.
%% First, we define operators
%% \[
%% P_{\Omega,B}(Y) = P_\Omega(Y)B' \text{ and } P_{\Omega,B}^\perp(Y) = P_\Omega^\perp(Y)B'
%% \]
%% Note that $P_{\Omega,B}$ is a linear operator and $P_{\Omega,B}(Y) + P_{\Omega,B}^\perp(Y) = YB'$. %, which is the only property of $P_\Omega$ required for directly applying proofs from \citep{mazumder2010spectral}. 
%% and
%% \begin{equation}
%%     \|P_{\Omega,B'}(Y) + P_{\Omega,B'}^\perp(Y)\|_F^2 = \|P_{\Omega,B'}(Y)\|_F^2 + \|P_{\Omega,B'}^\perp(Y)\|_F^2.
%% \end{equation} We define
Following their notation we define
\begin{align}\label{eq:helper-function}
 f_\lambda(W) = \frac{1}{2} \|P_{\Omega}(Y) - P_{\Omega}(WB')\|_F^2 + \lambda\|W\|_*.
\end{align}
Our objective is to find $ W_\lambda = {\arg\min}_W f_\lambda(W)$. We define
\begin{align}\label{eq:q}
Q_\lambda(W|\tilde{W}) = \frac{1}{2}\|P_{\Omega}(Y) + P_{\Omega}^{\perp}(\tilde{W} B') - WB'\|_F^2 + \lambda \|W\|_*.
\end{align}
%as a surrogate of $f_\lambda$.
We first show that Algorithm \ref{alg:soft-impute} in the step $k$ computes $W_\lambda^{k+1} = \argmin_W Q_\lambda(W|W_\lambda^k)$. Next we show that $W_\lambda^k$ converges to the solution of \eqref{eq:helper-function}, i.e. $W_\lambda^k \rightarrow W_\lambda$.
\begin{lemma}\label{lemma:svt}
Let $W$ be an $N \times K$ matrix of rank $r \leq K$ and $B$ is an orthogonal $T \times K$ matrix. The solution to the optimization problem
\begin{equation}\label{eq:lemma1}
\min_W \frac{1}{2}\|Y - WB' \|_F^2 + \lambda\|W\|_*
\end{equation}
is given by $\hat{W} = S_\lambda(YB)$ where
\[
S_\lambda(YB) \equiv WD_\lambda V' \text{ with } D_\lambda = \diag[(d_1 - \lambda)_+, ..., (d_r - \lambda)_+],
\]
$WDV'$ is the SVD of $YB$, $D = \diag[d_1,...,d_r]$, and $t_+ = \max(t,0)$.
\end{lemma}
\begin{proof}
% Note that for any $A_{N\times T}$ of rank $K$, we have $S_\lambda(AB) = S_\lambda(A)B$ and since \eqref{eq:lemma1} is equivalent to
% \[
% \min_Z \frac{1}{2}\|WB' - Z \|_F^2 + \lambda\|W\|_*,
% \]
By Lemma 1 from \citet{mazumder2010spectral} we know that $S_\lambda(YB)$ solves
\[
\min_W \frac{1}{2}\|YB - W \|_F^2 + \lambda\|W\|_*.
\]
Now, since we have $\|Y - WB' \|_F = \|YB - W \|_F$, $S_\lambda(YB)$ also solves our Lemma~\ref{lemma:svt}.
\end{proof}
\begin{lemma}\label{eq:z-sequence}
For every fixed $\lambda \geq 0$, define a sequence $W_\lambda^k$ by
\[
W_\lambda^{k+1} = \argmin_W Q_\lambda(W|W_\lambda^k)
\]
with any starting point $W_\lambda^0$. The sequence $W_\lambda^k$ satisfies 
\[
f_\lambda(W_\lambda^{k+1}) \leq  Q_\lambda(W_\lambda^{k+1} | W_\lambda^k ) \leq f_\lambda(W_\lambda^{k})
\]
\end{lemma}
\begin{proof}
By Lemma \ref{lemma:svt} and the definition \eqref{eq:q}, we have:
\begin{align*}
  f_\lambda(W_\lambda^k) &= Q_\lambda(W_\lambda^k | W_\lambda^k)\\
  &= \frac{1}{2}\|P_{\Omega}(Y) + P_{\Omega}^{\perp}(W_\lambda^k B') - W_\lambda^kB'\|_F^2 + \lambda \|W_\lambda^k\|_*\\
  &\geq \min_W \frac{1}{2}\|P_{\Omega}(Y) + P_{\Omega}^{\perp}(W_\lambda^k B') - WB'\|_F^2 + \lambda \|W\|_*\\
  &= Q_\lambda(W_\lambda^{k+1} | W_\lambda^k)\\
  &= \frac{1}{2}\|P_{\Omega}(Y) + P_{\Omega}^{\perp}(W_\lambda^k B') - W_\lambda^{k+1}B'\|_F^2 + \lambda \|W_\lambda^{k+1}\|_*\\
  &= \frac{1}{2}\|(P_{\Omega}(Y) - P_\Omega(W_\lambda^{k+1}B'))+ (P_{\Omega}^{\perp}(W_\lambda^k B') - P_\Omega^\perp(W_\lambda^{k+1}B'))\|_F^2 + \lambda \|W_\lambda^{k+1}\|_*\\
  &= \frac{1}{2}\|P_{\Omega}(Y) - P_\Omega(W_\lambda^{k+1}B')\|_F^2 + \frac{1}{2}\|P_{\Omega}^{\perp}(W_\lambda^k B') - P_\Omega^\perp(W_\lambda^{k+1}B')\|_F^2 + \lambda \|W_\lambda^{k+1}\|_*\\
  &\geq \frac{1}{2}\|P_{\Omega}(Y) - P_\Omega(W_\lambda^{k+1}B')\|_F^2 + \lambda \|W_\lambda^{k+1}\|_*\\
  &= Q_\lambda(W_\lambda^{k+1} | W_\lambda^{k+1})\\
  &= f(W_\lambda^{k+1}).
\end{align*}
\end{proof}
    Note that proofs of Lemma \ref{lemma:svt} and Lemma \ref{eq:z-sequence} are just elementary extentions of their counterparts in \citet{mazumder2010spectral}. Similarly, we get equivalent results for their Lemma 3-5 and the main theorem.
\begin{theorem}
The sequence of $W_{\lambda}^k$ defined in Lemma \ref{eq:z-sequence} converges to a limit $W_\lambda^\infty$ that solves
\[
\min_W \frac{1}{2} \|P_\Omega(Y) - P_\Omega(WB)\|_F^2 + \lambda\|W\|_*.
\]
\end{theorem}

%% \begin{proof}
%% It's sufficient to show that $W_{\lambda}^k$ has a limit. Then, the convergence follows from Lemma 5 from \citet{mazumder2010spectral}.

%% Now
%% \begin{align*}
%% \|W_\lambda - W_\lambda^k\|_F^2 &= \|S_\lambda((P_\Omega(Y) + P_\Omega^\perp(W_\lambda B))B') - S_\lambda((P_\Omega(Y) + P_\Omega^\perp(W_\lambda^{k-1}B))B')\|_F^2 \\
%% &\leq \|(P_\Omega(Y) + P_\Omega^\perp(W_\lambda B))B' - (P_\Omega(Y) + P_\Omega^\perp(W_\lambda^{k-1}B))B'\|_F^2\\
%% &= \|P_\Omega^\perp(W_\lambda B) - P_\Omega^\perp(W_\lambda^{k-1}B)\|_F^2\\
%% &= \|P_\Omega^\perp(W_\lambda B - W_\lambda^{k-1}B)\|_F^2\\
%% &\leq \|W_\lambda - W_\lambda^{k-1}\|_F^2
%% \end{align*}
%% and the reminder of the proof follows from the proof of Theorem 1 from \citet{mazumder2010spectral}.
%% \end{proof}

%% \section{\texttt{fimpute} package}
%% Let \verb|data| be the data matrix with columns \verb|subjectID, time, gdi, weight, dmc|, with $2-10$ observations for each \verb|subjectID|. Package \verb|fimpute| enables performing three tasks:
%% \begin{enumerate}
%% \item imputing missing values in \verb|gdi|,
%% \item projecting patients on a 2D plane using entire trajectories,
%% \item imputing missing values in \verb|gdi| using variables \verb|weight, dmc|.
%% \end{enumerate}
%% Each of these methods is embedded in the \verb|fregression(formula, data)| function, where the \verb|formula| can take one of the following forms, mimicking \verb|nlme| package:
%% \begin{enumerate}
%% \item \verb@gdi:time ~ 1 | subjectID@ for \textsc{Soft-Longitudinal-Impute},
%% \item \verb@gdi:1 ~ time + weight + dmc | subjectID@ for dimensionality reduction,
%% \item \verb@gdi:time ~ weight + dmc | subjectID@ for regression. 
%% \end{enumerate}
\end{document}
